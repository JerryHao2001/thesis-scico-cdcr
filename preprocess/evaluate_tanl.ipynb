{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd75fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have datasets installed:\n",
    "\n",
    "from datasets import load_dataset\n",
    "import json, re, os\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# --- config ---\n",
    "SPLIT = \"train\"           # \"train\" | \"validation\" | \"test\"\n",
    "K = 3                    # how many topics to examine\n",
    "PRED_PATH = \"scico_train_tanl_extraction.jsonl\"  # path to your cached predictions JSONL\n",
    "SHOW_FIRST_N_PARAS = 3   # per topic, show this many paragraphs verbosely\n",
    "SHOW_FIRST_N_MENTIONS = 10  # truncate long lists for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8965709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must match your extractor's detok to align strings\n",
    "_punct = re.compile(r\"\\s+([,.;:%)\\]])\")\n",
    "_openp = re.compile(r\"([\\[(])\\s+\")\n",
    "def detok(tokens: List[str]) -> str:\n",
    "    s = \" \".join(tokens)\n",
    "    s = _punct.sub(r\"\\1\", s)\n",
    "    s = _openp.sub(r\"\\1\", s)\n",
    "    return s\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = s.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"')\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "# bracketed: [ mention | type | rels ... ]\n",
    "BRACKET_SPAN_RE = re.compile(r\"\\[\\s*(?P<mention>.+?)\\s*\\|\\s*[^|\\]]+?(?:\\s*\\|[^\\]]+)?\\]\")\n",
    "\n",
    "def extract_pred_mentions(tanl_output: str) -> List[str]:\n",
    "    out = []\n",
    "    for m in BRACKET_SPAN_RE.finditer(tanl_output or \"\"):\n",
    "        raw = m.group(\"mention\")\n",
    "        # be defensive if mention text accidentally contains a '|'\n",
    "        out.append(raw.split(\"|\")[0].strip())\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a56c2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "scico = load_dataset(\"allenai/scico\")[SPLIT]\n",
    "\n",
    "# For fast lookup: (topic_id, para_idx) -> {\"text\":..., \"gold_mentions\":[...], \"doc_id\":...}\n",
    "gold = {}\n",
    "for row in scico:\n",
    "    tid = int(row[\"id\"])\n",
    "    for pidx, toks in enumerate(row[\"tokens\"]):\n",
    "        text = detok(toks)\n",
    "        # collect gold mentions for this paragraph\n",
    "        gstrings = []\n",
    "        for (pid, s, e, _cid) in row[\"mentions\"]:\n",
    "            if pid == pidx:\n",
    "                gstrings.append(detok(row[\"tokens\"][pid][s:e]))\n",
    "        gold[(tid, pidx)] = {\n",
    "            \"text\": text,\n",
    "            \"gold_mentions\": gstrings,\n",
    "            \"doc_id\": int(row[\"doc_ids\"][pidx]),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66a105c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold paragraphs: 10,660 | Predicted paragraphs: 10,660 | Overlap: 10,660\n"
     ]
    }
   ],
   "source": [
    "assert os.path.exists(PRED_PATH), f\"Missing predictions file at {PRED_PATH}\"\n",
    "\n",
    "pred = {}  # (tid, pidx) -> record\n",
    "with open(PRED_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec = json.loads(line)\n",
    "        key = (int(rec[\"topic_id\"]), int(rec[\"para_idx\"]))\n",
    "        pred[key] = rec\n",
    "\n",
    "# quick sanity: how many overlaps\n",
    "overlap = sum(1 for k in gold if k in pred)\n",
    "print(f\"Gold paragraphs: {len(gold):,} | Predicted paragraphs: {len(pred):,} | Overlap: {overlap:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a7736d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_paragraph(tid: int, pidx: int, show_text: bool = False) -> Dict:\n",
    "    g = gold.get((tid, pidx), None)\n",
    "    pr = pred.get((tid, pidx), None)\n",
    "    if g is None:\n",
    "        return {\"status\": \"missing-gold\"}\n",
    "    if pr is None:\n",
    "        return {\"status\": \"missing-pred\"}\n",
    "\n",
    "    text_g = g[\"text\"]\n",
    "    text_p = pr.get(\"text\", \"\")\n",
    "    same_text = (norm(text_g) == norm(text_p))\n",
    "\n",
    "    # gold mentions (strings)\n",
    "    gold_list = g[\"gold_mentions\"]\n",
    "    # predicted mentions (strings extracted from TANL brackets)\n",
    "    pred_list = extract_pred_mentions(pr.get(\"tanl_output\",\"\"))\n",
    "\n",
    "    # normalize for equality check (we keep raw for display)\n",
    "    gold_norm = set(norm(x) for x in gold_list)\n",
    "    pred_norm = set(norm(x) for x in pred_list)\n",
    "\n",
    "    hits = [m for m in gold_list if norm(m) in pred_norm]\n",
    "    miss = [m for m in gold_list if norm(m) not in pred_norm]\n",
    "\n",
    "    out = {\n",
    "        \"status\": \"ok\",\n",
    "        \"tid\": tid,\n",
    "        \"para_idx\": pidx,\n",
    "        \"doc_id\": g[\"doc_id\"],\n",
    "        \"texts_equal\": same_text,\n",
    "        \"n_gold\": len(gold_list),\n",
    "        \"n_pred\": len(pred_list),\n",
    "        \"n_hits\": len(hits),\n",
    "        \"n_miss\": len(miss),\n",
    "        \"hits\": hits,\n",
    "        \"miss\": miss,\n",
    "        \"gold_sample\": gold_list[:SHOW_FIRST_N_MENTIONS],\n",
    "        \"pred_sample\": pred_list[:SHOW_FIRST_N_MENTIONS],\n",
    "    }\n",
    "    if show_text:\n",
    "        out[\"text\"] = text_g\n",
    "        out[\"tanl_output\"] = pred.get(\"tanl_output\",\"\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b25dadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "[1/3] Topic 521 — 66 paragraphs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Topic-level mention recall: 2/67 = 0.030  | predicted mentions in topic: 385\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "texts_equal_to_pred_text: True\n",
      "\n",
      "[TEXT]\n",
      "With the rapid development of the network technologies, software development is becoming more and more complicated. Traditional software engineering management methods based on Client/Server structure have not been very competent for large-scale software development.\n",
      "\n",
      "[GOLD mentions] (n = 1 ) → ['software']\n",
      "[PRED mentions] (n = 5 ) → ['network technologies', 'software development', 'software engineering management methods', 'Client/Server structure', 'large-scale software development']\n",
      "[HITS] → []\n",
      "[MISS] → ['software']\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "texts_equal_to_pred_text: True\n",
      "\n",
      "[TEXT]\n",
      "Conclusion: Selecting the appropriate integration architecture is a fundamental issue of any software development project. HIS-DF provides a unique methodological approach guiding the development of healthcare integration projects.\n",
      "\n",
      "[GOLD mentions] (n = 1 ) → ['software development']\n",
      "[PRED mentions] (n = 5 ) → ['integration architecture', 'software development project', 'HIS-DF', 'methodological approach', 'healthcare integration projects']\n",
      "[HITS] → []\n",
      "[MISS] → ['software development']\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "texts_equal_to_pred_text: True\n",
      "\n",
      "[TEXT]\n",
      "Between Communication Activities and Success Indexes in Small and Medium Software Projects. In the field of software developing, project is almost the most common organizational form. But with the enhancement of software products ' complexity and the constant change of customers ' demand, the importance of communication in software development projects is increasingly prominent.\n",
      "\n",
      "[GOLD mentions] (n = 1 ) → ['software']\n",
      "[PRED mentions] (n = 0 ) → []\n",
      "[HITS] → []\n",
      "[MISS] → ['software']\n",
      "====================================================================================================\n",
      "[2/3] Topic 522 — 51 paragraphs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Topic-level mention recall: 2/53 = 0.038  | predicted mentions in topic: 404\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "texts_equal_to_pred_text: True\n",
      "\n",
      "[TEXT]\n",
      "It communicate from one vehicle (source) to another vehicle (destination) through different vehicles (intermediate nodes). A numbers of different routing protocols for communication, ie multimedia data, text data etc. from one vehicle (node) to another vehicle are existing.\n",
      "\n",
      "[GOLD mentions] (n = 1 ) → ['routing']\n",
      "[PRED mentions] (n = 4 ) → ['routing protocols', 'communication', 'multimedia data', 'text data']\n",
      "[HITS] → []\n",
      "[MISS] → ['routing']\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "texts_equal_to_pred_text: True\n",
      "\n",
      "[TEXT]\n",
      "MAC layer utilises the cluster tree formula to establish tree configuration. Subsequently, that cluster table is taken to the routing layer protocol of cross layer ZigBee-based routing protocol (CLZBRP). Here CLZBRP is compared with ad hoc on-demand distance vector routing (AODV) and tree-routing (TR) with QOS.\n",
      "\n",
      "[GOLD mentions] (n = 1 ) → ['routing layer']\n",
      "[PRED mentions] (n = 9 ) → ['MAC layer', 'cluster tree formula', 'tree configuration', 'cluster table', 'routing layer protocol', 'cross layer ZigBee-based routing protocol (CLZBRP)', 'CLZBRP', 'ad hoc on-demand distance vector routing (AODV) and tree-routing (TR)', 'QOS']\n",
      "[HITS] → []\n",
      "[MISS] → ['routing layer']\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "texts_equal_to_pred_text: True\n",
      "\n",
      "[TEXT]\n",
      "At routing layer every node gathers the number of packets and bytes awaiting transmission at MAC layers of nodes that are located within its circle of ' safe-distance ' and then finds congestion free routes. In addition, the routing layer protocol finds multiple node-disjoint paths for every source destination pair that is separated by a minimum of ' safe- distance ' except for the nodes located within ' safe-distance ' of source and destination nodes. The protocol also perform local repair of existing routes thus providing a good degree of safeguard against mobility of nodes.\n",
      "\n",
      "[GOLD mentions] (n = 1 ) → ['routing layer']\n",
      "[PRED mentions] (n = 4 ) → ['routing layer', 'routing layer protocol', 'node-disjoint paths', 'protocol']\n",
      "[HITS] → ['routing layer']\n",
      "[MISS] → []\n",
      "====================================================================================================\n",
      "[3/3] Topic 523 — 57 paragraphs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Topic-level mention recall: 2/59 = 0.034  | predicted mentions in topic: 648\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "texts_equal_to_pred_text: True\n",
      "\n",
      "[TEXT]\n",
      "In this paper, we focus on the feature pooling methods for scene character recognition. We research three kinds of pooling methods: the average (sum) pooling, max pooling and weighted-based pooling methods. Specifically, various feature pooling methods are introduced, their merits and demerits are studied, and existing problems are discussed.\n",
      "\n",
      "[GOLD mentions] (n = 1 ) → ['pooling']\n",
      "[PRED mentions] (n = 7 ) → ['feature pooling methods', 'scene character recognition', 'pooling methods', 'average (sum) pooling', 'max pooling', 'weighted-based pooling methods', 'feature pooling methods']\n",
      "[HITS] → []\n",
      "[MISS] → ['pooling']\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "texts_equal_to_pred_text: True\n",
      "\n",
      "[TEXT]\n",
      "In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets; they are also easy to implement, and can be applied within various deep neural network architectures. These benefits come with only a light increase in computational overhead during training and a very modest increase in the number of model parameters.\n",
      "\n",
      "[GOLD mentions] (n = 1 ) → ['pooling']\n",
      "[PRED mentions] (n = 8 ) → ['generalized pooling operation', 'average or max pooling', 'pooling operations', 'invariance properties', 'benchmark datasets', 'they', 'deep neural network architectures', 'computational overhead']\n",
      "[HITS] → []\n",
      "[MISS] → ['pooling']\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "texts_equal_to_pred_text: True\n",
      "\n",
      "[TEXT]\n",
      "This way, a series of representations can be derived that trade-off generalization against specificity. Thereby we show a connection between NBNN classification and different pooling strategies, where, in contrast to traditional pooling schemes that perform spatial pooling of the features, pooling is performed in feature space. Moreover, rather than picking a single partitioning, we propose to combine them in a multi kernel framework.\n",
      "\n",
      "[GOLD mentions] (n = 1 ) → ['pooling']\n",
      "[PRED mentions] (n = 10 ) → ['representations', 'trade-off generalization', 'NBNN classification', 'pooling strategies', 'pooling schemes', 'spatial pooling of the features', 'pooling', 'feature space', 'them', 'multi kernel framework']\n",
      "[HITS] → ['pooling']\n",
      "[MISS] → []\n"
     ]
    }
   ],
   "source": [
    "# pick the first K topics from the dataset order\n",
    "topic_ids = [int(scico[i][\"id\"]) for i in range(min(K, len(scico)))]\n",
    "\n",
    "for t_idx, tid in enumerate(topic_ids, 1):\n",
    "    row = next(r for r in scico if int(r[\"id\"]) == tid)\n",
    "    n_paras = len(row[\"tokens\"])\n",
    "    print(\"=\"*100)\n",
    "    print(f\"[{t_idx}/{len(topic_ids)}] Topic {tid} — {n_paras} paragraphs\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    # summarize counts per topic\n",
    "    topic_hits = topic_gold = topic_pred_mentions = 0\n",
    "    for pidx in range(n_paras):\n",
    "        g = gold.get((tid,pidx))\n",
    "        pr = pred.get((tid,pidx))\n",
    "        if g is None or pr is None:\n",
    "            continue\n",
    "        res = compare_paragraph(tid, pidx)\n",
    "        topic_hits += res[\"n_hits\"]\n",
    "        topic_gold += res[\"n_gold\"]\n",
    "        topic_pred_mentions += res[\"n_pred\"]\n",
    "\n",
    "    recall = (topic_hits/topic_gold) if topic_gold else 0.0\n",
    "    print(f\"Topic-level mention recall: {topic_hits}/{topic_gold} = {recall:.3f}  | predicted mentions in topic: {topic_pred_mentions}\")\n",
    "\n",
    "    # show the first few paragraphs verbosely\n",
    "    for pidx in range(min(SHOW_FIRST_N_PARAS, n_paras)):\n",
    "        res = compare_paragraph(tid, pidx, show_text=True)\n",
    "        print(\"\\n--- Paragraph\", pidx, f\"(doc_id={res.get('doc_id','?')}) ---\")\n",
    "        if res[\"status\"] != \"ok\":\n",
    "            print(res[\"status\"])\n",
    "            continue\n",
    "\n",
    "        print(f\"texts_equal_to_pred_text: {res['texts_equal']}\")\n",
    "        print(\"\\n[TEXT]\")\n",
    "        print(res[\"text\"])\n",
    "        print(\"\\n[GOLD mentions] (n =\", res[\"n_gold\"], \") →\", res[\"gold_sample\"])\n",
    "        print(\"[PRED mentions] (n =\", res[\"n_pred\"], \") →\", res[\"pred_sample\"])\n",
    "        print(\"[HITS] →\", res[\"hits\"])\n",
    "        print(\"[MISS] →\", res[\"miss\"])\n",
    "        # optionally also show the full TANL output for this paragraph\n",
    "        # print(\"\\n[TANL OUTPUT]\")\n",
    "        # print(res[\"tanl_output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05e932ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\n",
    "  \"With\",\n",
    "  \"the\",\n",
    "  \"rapid\",\n",
    "  \"development\",\n",
    "  \"of\",\n",
    "  \"the\",\n",
    "  \"network\",\n",
    "  \"technologies\",\n",
    "  \",\",\n",
    "  \"software\",\n",
    "  \"development\",\n",
    "  \"is\",\n",
    "  \"becoming\",\n",
    "  \"more\",\n",
    "  \"and\",\n",
    "  \"more\",\n",
    "  \"complicated\",\n",
    "  \".\",\n",
    "  \"Traditional\",\n",
    "  \"software\",\n",
    "  \"engineering\",\n",
    "  \"management\",\n",
    "  \"methods\",\n",
    "  \"based\",\n",
    "  \"on\",\n",
    "  \"Client/Server\",\n",
    "  \"structure\",\n",
    "  \"have\",\n",
    "  \"not\",\n",
    "  \"been\",\n",
    "  \"very\",\n",
    "  \"competent\",\n",
    "  \"for\",\n",
    "  \"large-scale\",\n",
    "  \"software\",\n",
    "  \"development\",\n",
    "  \".\",\n",
    "  \"Conclusion\",\n",
    "  \":\",\n",
    "  \"Selecting\",\n",
    "  \"the\",\n",
    "  \"appropriate\",\n",
    "  \"integration\",\n",
    "  \"architecture\",\n",
    "  \"is\",\n",
    "  \"a\",\n",
    "  \"fundamental\",\n",
    "  \"issue\",\n",
    "  \"of\",\n",
    "  \"any\",\n",
    "  \"software\",\n",
    "  \"development\",\n",
    "  \"project\",\n",
    "  \".\",\n",
    "  \"HIS-DF\",\n",
    "  \"provides\",\n",
    "  \"a\",\n",
    "  \"unique\",\n",
    "  \"methodological\",\n",
    "  \"approach\",\n",
    "  \"guiding\",\n",
    "  \"the\",\n",
    "  \"development\",\n",
    "  \"of\",\n",
    "  \"healthcare\",\n",
    "  \"integration\",\n",
    "  \"projects\",\n",
    "  \".\",\n",
    "  \"Between\",\n",
    "  \"Communication\",\n",
    "  \"Activities\",\n",
    "  \"and\",\n",
    "  \"Success\",\n",
    "  \"Indexes\",\n",
    "  \"in\",\n",
    "  \"Small\",\n",
    "  \"and\",\n",
    "  \"Medium\",\n",
    "  \"Software\",\n",
    "  \"Projects\",\n",
    "  \".\",\n",
    "  \"In\",\n",
    "  \"the\",\n",
    "  \"field\",\n",
    "  \"of\",\n",
    "  \"software\",\n",
    "  \"developing\",\n",
    "  \",\",\n",
    "  \"project\",\n",
    "  \"is\",\n",
    "  \"almost\",\n",
    "  \"the\",\n",
    "  \"most\",\n",
    "  \"common\",\n",
    "  \"organizational\",\n",
    "  \"form\",\n",
    "  \".\",\n",
    "  \"But\",\n",
    "  \"with\",\n",
    "  \"the\",\n",
    "  \"enhancement\",\n",
    "  \"of\",\n",
    "  \"software\",\n",
    "  \"products\",\n",
    "  \"'\",\n",
    "  \"complexity\",\n",
    "  \"and\",\n",
    "  \"the\",\n",
    "  \"constant\",\n",
    "  \"change\",\n",
    "  \"of\",\n",
    "  \"customers\",\n",
    "  \"'\",\n",
    "  \"demand\",\n",
    "  \",\",\n",
    "  \"the\",\n",
    "  \"importance\",\n",
    "  \"of\",\n",
    "  \"communication\",\n",
    "  \"in\",\n",
    "  \"software\",\n",
    "  \"development\",\n",
    "  \"projects\",\n",
    "  \"is\",\n",
    "  \"increasingly\",\n",
    "  \"prominent\",\n",
    "  \".\",\n",
    "  \"Software\",\n",
    "  \"reuse\",\n",
    "  \"is\",\n",
    "  \"a\",\n",
    "  \"technology\",\n",
    "  \"that\",\n",
    "  \"is\",\n",
    "  \"usually\",\n",
    "  \"used\",\n",
    "  \"in\",\n",
    "  \"software\",\n",
    "  \"developing\",\n",
    "  \".\",\n",
    "  \"This\",\n",
    "  \"paper\",\n",
    "  \"discusses\",\n",
    "  \"why\",\n",
    "  \"object\",\n",
    "  \"oriented\",\n",
    "  \"programming\",\n",
    "  \"(\",\n",
    "  \"OOP\",\n",
    "  \")\",\n",
    "  \"are\",\n",
    "  \"suitable\",\n",
    "  \"for\",\n",
    "  \"supporting\",\n",
    "  \"software\",\n",
    "  \"reuse\",\n",
    "  \"and\",\n",
    "  \"states\",\n",
    "  \"the\",\n",
    "  \"ways\",\n",
    "  \"of\",\n",
    "  \"implementing\",\n",
    "  \"software\",\n",
    "  \"reuse\",\n",
    "  \".\",\n",
    "  \"Reverse\",\n",
    "  \"engineering\",\n",
    "  \"a\",\n",
    "  \"program\",\n",
    "  \"constructs\",\n",
    "  \"a\",\n",
    "  \"high-level\",\n",
    "  \"representation\",\n",
    "  \"suitable\",\n",
    "  \"for\",\n",
    "  \"various\",\n",
    "  \"software\",\n",
    "  \"development\",\n",
    "  \"purposes\",\n",
    "  \"such\",\n",
    "  \"as\",\n",
    "  \"documentation\",\n",
    "  \"or\",\n",
    "  \"reengineering\",\n",
    "  \".\",\n",
    "  \"Unfortunately\",\n",
    "  \"however\",\n",
    "  \",\",\n",
    "  \"there\",\n",
    "  \"are\",\n",
    "  \"no\",\n",
    "  \"established\",\n",
    "  \"guidelines\",\n",
    "  \"to\",\n",
    "  \"assess\",\n",
    "  \"the\",\n",
    "  \"adequacy\",\n",
    "  \"of\",\n",
    "  \"such\",\n",
    "  \"a\",\n",
    "  \"representation\",\n",
    "  \".\",\n",
    "  \"Polymorphism\",\n",
    "  \"is\",\n",
    "  \"the\",\n",
    "  \"ability\",\n",
    "  \"of\",\n",
    "  \"two\",\n",
    "  \"classes\",\n",
    "  \"to\",\n",
    "  \"react\",\n",
    "  \"differently\",\n",
    "  \"to\",\n",
    "  \"the\",\n",
    "  \"same\",\n",
    "  \"message\",\n",
    "  \".\",\n",
    "  \"G.\",\n",
    "  \"Software\",\n",
    "  \"Development\",\n",
    "  \"Software\",\n",
    "  \"development\",\n",
    "  \"is\",\n",
    "  \"similar\",\n",
    "  \"to\",\n",
    "  \"most\",\n",
    "  \"other\",\n",
    "  \"types\",\n",
    "  \"of\",\n",
    "  \"construction\",\n",
    "  \"processes\",\n",
    "  \".\",\n",
    "  \"A\",\n",
    "  \"complex\",\n",
    "  \"problem\",\n",
    "  \"is\",\n",
    "  \"encountered\",\n",
    "  \";\",\n",
    "  \"a\",\n",
    "  \"solution\",\n",
    "  \"is\",\n",
    "  \"deduced\",\n",
    "  \";\",\n",
    "  \"then\",\n",
    "  \"construction\",\n",
    "  \"of\",\n",
    "  \"the\",\n",
    "  \"solution\",\n",
    "  \"occurs\",\n",
    "  \".\",\n",
    "  \"The\",\n",
    "  \"method\",\n",
    "  \"used\",\n",
    "  \"in\",\n",
    "  \"the\",\n",
    "  \"applicationdevelopment\",\n",
    "  \"approach\",\n",
    "  \"is\",\n",
    "  \"a\",\n",
    "  \"combination\",\n",
    "  \"of\",\n",
    "  \"interactive\",\n",
    "  \"multimedia\",\n",
    "  \"and\",\n",
    "  \"educational\",\n",
    "  \"psychology\",\n",
    "  \".\",\n",
    "  \"When\",\n",
    "  \"thesoftware\",\n",
    "  \"development\",\n",
    "  \",\",\n",
    "  \"we\",\n",
    "  \"consider\",\n",
    "  \"several\",\n",
    "  \"aspects\",\n",
    "  \"e.g\",\n",
    "  \":\",\n",
    "  \"interfaces\",\n",
    "  \",\",\n",
    "  \"interactivity\",\n",
    "  \",\",\n",
    "  \"ease\",\n",
    "  \"of\",\n",
    "  \"use\",\n",
    "  \",\",\n",
    "  \"and\",\n",
    "  \"standalone\",\n",
    "  \"software\",\n",
    "  \"running\",\n",
    "  \"on\",\n",
    "  \"mobile\",\n",
    "  \"phones\",\n",
    "  \"and\",\n",
    "  \"multimedia-based\",\n",
    "  \".\",\n",
    "  \"Furthermore\",\n",
    "  \",\",\n",
    "  \"to\",\n",
    "  \"test\",\n",
    "  \"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "380e6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9744768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- configuration ----------\n",
    "SPLIT = \"train\"                        # \"train\" | \"validation\" | \"test\"\n",
    "K = 30                                  # how many topics (rows) to examine step by step\n",
    "PRED_PATH = \"scico_train_tanl_extraction.jsonl\"  # produced by the batch extractor\n",
    "\n",
    "# fuzzy match thresholds\n",
    "DELTA_POS = 2   # allow predicted start token to be within ±2 of gold start\n",
    "DELTA_LEN = 2   # allow predicted length to differ by at most 2 tokens\n",
    "\n",
    "# how much to print\n",
    "SHOW_FIRST_N_PARAS = 4\n",
    "SHOW_FIRST_N_PRED  = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cf735fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detokenize exactly like we did at extraction time\n",
    "_punct = re.compile(r\"\\s+([,.;:%)\\]])\")\n",
    "_openp = re.compile(r\"([\\[(])\\s+\")\n",
    "def detok(tokens: List[str]) -> str:\n",
    "    s = \" \".join(tokens)\n",
    "    s = _punct.sub(r\"\\1\", s)\n",
    "    s = _openp.sub(r\"\\1\", s)\n",
    "    return s\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = s.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"')\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "# extract the *mention phrase* (the bit before the first \"|\") from TANL brackets\n",
    "BRACKET_SPAN_RE = re.compile(r\"\\[\\s*(?P<mention>.+?)\\s*\\|\\s*[^|\\]]+?(?:\\s*\\|[^\\]]+)?\\]\")\n",
    "\n",
    "def extract_pred_mentions(tanl_output: str) -> List[str]:\n",
    "    out = []\n",
    "    for m in BRACKET_SPAN_RE.finditer(tanl_output or \"\"):\n",
    "        raw = m.group(\"mention\")\n",
    "        out.append(raw.split(\"|\")[0].strip())\n",
    "    return out\n",
    "\n",
    "# map a predicted mention string back to token spans by exact detok-equality over windows\n",
    "def find_token_spans_for_string(par_tokens: List[str], mention: str, max_window: int = 20) -> List[Tuple[int,int]]:\n",
    "    tgt = norm(mention)\n",
    "    N = len(par_tokens)\n",
    "    hits = []\n",
    "    # cap window length to avoid quadratic blowup; mentions are short by nature\n",
    "    for i in range(N):\n",
    "        acc = []\n",
    "        for j in range(i+1, min(N, i+max_window)+1):\n",
    "            acc.append(par_tokens[j-1])\n",
    "            cand = norm(detok(acc))\n",
    "            if cand == tgt:\n",
    "                hits.append((i, j))  # [start, end)\n",
    "                break\n",
    "            if len(cand) > len(tgt) + 10:\n",
    "                break\n",
    "    return hits\n",
    "\n",
    "def fuzzy_hit(gold_span: Tuple[int,int], pred_span: Tuple[int,int], delta_pos=2, delta_len=2) -> bool:\n",
    "    gs, ge = gold_span\n",
    "    ps, pe = pred_span\n",
    "    len_g = ge - gs\n",
    "    len_p = pe - ps\n",
    "    return abs(ps - gs) <= delta_pos and abs(len_p - len_g) <= delta_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42fdc557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SciCo train: topics=221, paragraphs=10660\n"
     ]
    }
   ],
   "source": [
    "scico = load_dataset(\"allenai/scico\")[SPLIT]\n",
    "\n",
    "# Build gold maps:\n",
    "#   (topic_id, para_idx) -> {\n",
    "#       \"tokens\": [...],\n",
    "#       \"doc_id\": int,\n",
    "#       \"gold_spans\": [(start,end), ...],\n",
    "#       \"gold_strings\": [\"...\", ...]\n",
    "#   }\n",
    "gold = {}\n",
    "order_topic_ids = []\n",
    "\n",
    "for i in range(len(scico)):\n",
    "    row = scico[i]\n",
    "    tid = int(row[\"id\"])\n",
    "    order_topic_ids.append(tid)\n",
    "    for pid, toks in enumerate(row[\"tokens\"]):\n",
    "        spans = []\n",
    "        strings = []\n",
    "        for (ppid, s, e, _cid) in row[\"mentions\"]:\n",
    "            if ppid == pid:\n",
    "                spans.append((int(s), int(e)))\n",
    "                strings.append(detok(row[\"tokens\"][pid][s:e+1]))\n",
    "        gold[(tid, pid)] = {\n",
    "            \"tokens\": row[\"tokens\"][pid],\n",
    "            \"doc_id\": int(row[\"doc_ids\"][pid]),\n",
    "            \"gold_spans\": spans,\n",
    "            \"gold_strings\": strings,\n",
    "        }\n",
    "\n",
    "print(f\"Loaded SciCo {SPLIT}: topics={len(scico)}, paragraphs={len(gold)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "febede44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted paragraphs: 10660\n"
     ]
    }
   ],
   "source": [
    "assert os.path.exists(PRED_PATH), f\"Missing predictions JSONL at {PRED_PATH}\"\n",
    "\n",
    "pred = {}  # (tid, pid) -> record dict\n",
    "with open(PRED_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec = json.loads(line)\n",
    "        key = (int(rec[\"topic_id\"]), int(rec[\"para_idx\"]))\n",
    "        pred[key] = rec\n",
    "\n",
    "print(f\"Predicted paragraphs: {len(pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0da443c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_paragraph_fuzzy(tid: int, pid: int, delta_pos=2, delta_len=2):\n",
    "    g = gold.get((tid, pid))\n",
    "    p = pred.get((tid, pid))\n",
    "    if g is None:\n",
    "        return {\"status\": \"missing-gold\"}\n",
    "    if p is None:\n",
    "        return {\"status\": \"missing-pred\"}\n",
    "\n",
    "    tokens = g[\"tokens\"]\n",
    "    gold_spans = g[\"gold_spans\"]\n",
    "    gold_strings = g[\"gold_strings\"]\n",
    "\n",
    "    # predicted raw mentions (strings from brackets)\n",
    "    pred_strings = extract_pred_mentions(p.get(\"tanl_output\",\"\"))\n",
    "\n",
    "    # map each predicted string to *all* matching token spans in this paragraph\n",
    "    pred_spans = []\n",
    "    for s in pred_strings:\n",
    "        spans = find_token_spans_for_string(tokens, s, max_window=20)\n",
    "        # keep all matches (rare to have >1, but possible)\n",
    "        pred_spans.extend(spans)\n",
    "\n",
    "    # for display: pair spans with strings (first match per string)\n",
    "    pred_span_for_display = []\n",
    "    seen = set()\n",
    "    for s in pred_strings:\n",
    "        spans = find_token_spans_for_string(tokens, s, max_window=20)\n",
    "        if spans:\n",
    "            if spans[0] not in seen:\n",
    "                pred_span_for_display.append((s, spans[0]))\n",
    "                seen.add(spans[0])\n",
    "\n",
    "    # greedy “found” counting: each gold may match at most one predicted span\n",
    "    hits = 0\n",
    "    misses = []\n",
    "    matched_pred = set()\n",
    "    for gidx, gs in enumerate(gold_spans):\n",
    "        found = False\n",
    "        for ps in pred_spans:\n",
    "            if ps in matched_pred:\n",
    "                continue\n",
    "            if fuzzy_hit(gs, ps, delta_pos=delta_pos, delta_len=delta_len):\n",
    "                hits += 1\n",
    "                matched_pred.add(ps)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            misses.append((gidx, gs, gold_strings[gidx] if gidx < len(gold_strings) else \"\"))\n",
    "\n",
    "    out = {\n",
    "        \"status\": \"ok\",\n",
    "        \"doc_id\": g[\"doc_id\"],\n",
    "        \"text\": detok(tokens),\n",
    "        \"gold_spans\": gold_spans,\n",
    "        \"gold_strings\": gold_strings,\n",
    "        \"pred_strings\": pred_strings,\n",
    "        \"pred_spans_display\": pred_span_for_display,\n",
    "        \"n_gold\": len(gold_spans),\n",
    "        \"n_pred\": len(pred_spans),\n",
    "        \"hits\": hits,\n",
    "        \"misses\": misses,\n",
    "    }\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8ce185b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "[1/30] Topic 521 — paragraphs: 66\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "With the rapid development of the network technologies, software development is becoming more and more complicated. Traditional software engineering management methods based on Client/Server structure have not been very competent for large-scale software development.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['software development']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['network technologies', 'software development', 'software engineering management methods', 'Client/Server structure', 'large-scale software development']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(6,8) :: network technologies', '(9,11) :: software development', '(19,23) :: software engineering management methods', '(25,27) :: Client/Server structure', '(33,36) :: large-scale software development']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "Conclusion: Selecting the appropriate integration architecture is a fundamental issue of any software development project. HIS-DF provides a unique methodological approach guiding the development of healthcare integration projects.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['software development project']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['integration architecture', 'software development project', 'HIS-DF', 'methodological approach', 'healthcare integration projects']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(5,7) :: integration architecture', '(13,16) :: software development project', '(17,18) :: HIS-DF', '(21,23) :: methodological approach', '(27,30) :: healthcare integration projects']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "Between Communication Activities and Success Indexes in Small and Medium Software Projects. In the field of software developing, project is almost the most common organizational form. But with the enhancement of software products ' complexity and the constant change of customers ' demand, the importance of communication in software development projects is increasingly prominent.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['software developing']\n",
      "\n",
      "[PRED mentions (strings)] n= 0\n",
      "[]\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "[]\n",
      "\n",
      "HITS=0  MISSES=1\n",
      "Missed gold (idx, span, text):\n",
      "  - #0 (17,18) :: software developing\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Software reuse is a technology that is usually used in software developing. This paper discusses why object oriented programming (OOP) are suitable for supporting software reuse and states the ways of implementing software reuse.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['software developing']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['Software reuse', 'software developing', 'object oriented programming ( OOP)', 'software reuse', 'software reuse']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,2) :: Software reuse', '(10,12) :: software developing']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 521 recall: 40/67 = 0.597\n",
      "==============================================================================================================\n",
      "[2/30] Topic 522 — paragraphs: 51\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "It communicate from one vehicle (source) to another vehicle (destination) through different vehicles (intermediate nodes). A numbers of different routing protocols for communication, ie multimedia data, text data etc. from one vehicle (node) to another vehicle are existing.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['routing protocols']\n",
      "\n",
      "[PRED mentions (strings)] n= 4\n",
      "['routing protocols', 'communication', 'multimedia data', 'text data']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(26,28) :: routing protocols', '(29,30) :: communication', '(32,34) :: multimedia data', '(35,37) :: text data']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "MAC layer utilises the cluster tree formula to establish tree configuration. Subsequently, that cluster table is taken to the routing layer protocol of cross layer ZigBee-based routing protocol (CLZBRP). Here CLZBRP is compared with ad hoc on-demand distance vector routing (AODV) and tree-routing (TR) with QOS.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['routing layer protocol']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['MAC layer', 'cluster tree formula', 'tree configuration', 'cluster table', 'routing layer protocol', 'cross layer ZigBee-based routing protocol (CLZBRP)', 'CLZBRP', 'ad hoc on-demand distance vector routing (AODV) and tree-routing (TR)', 'QOS']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,2) :: MAC layer', '(4,7) :: cluster tree formula', '(9,11) :: tree configuration', '(15,17) :: cluster table', '(21,24) :: routing layer protocol', '(25,33) :: cross layer ZigBee-based routing protocol (CLZBRP)', '(31,32) :: CLZBRP', '(39,53) :: ad hoc on-demand distance vector routing (AODV) and tree-routing (TR)', '(54,55) :: QOS']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "At routing layer every node gathers the number of packets and bytes awaiting transmission at MAC layers of nodes that are located within its circle of ' safe-distance ' and then finds congestion free routes. In addition, the routing layer protocol finds multiple node-disjoint paths for every source destination pair that is separated by a minimum of ' safe- distance ' except for the nodes located within ' safe-distance ' of source and destination nodes. The protocol also perform local repair of existing routes thus providing a good degree of safeguard against mobility of nodes.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['routing layer protocol']\n",
      "\n",
      "[PRED mentions (strings)] n= 4\n",
      "['routing layer', 'routing layer protocol', 'node-disjoint paths', 'protocol']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(1,3) :: routing layer', '(40,43) :: routing layer protocol', '(45,47) :: node-disjoint paths', '(42,43) :: protocol']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "In the paper the authors show that this concept has the advantage of providing a means of improving the survivability of virtual channels. To this end, they present a simple rerouting protocol, which can be invoked upon the failure of an intermediate link or node of a virtual path. This protocol reroutes all the affected virtual channels to an alternative virtual path.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['rerouting protocol']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['concept', 'survivability of virtual channels', 'rerouting protocol', 'protocol', 'virtual path']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(8,9) :: concept', '(19,23) :: survivability of virtual channels', '(32,34) :: rerouting protocol', '(33,34) :: protocol', '(50,52) :: virtual path']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 522 recall: 46/53 = 0.868\n",
      "==============================================================================================================\n",
      "[3/30] Topic 523 — paragraphs: 57\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "In this paper, we focus on the feature pooling methods for scene character recognition. We research three kinds of pooling methods: the average (sum) pooling, max pooling and weighted-based pooling methods. Specifically, various feature pooling methods are introduced, their merits and demerits are studied, and existing problems are discussed.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['pooling methods']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['feature pooling methods', 'scene character recognition', 'pooling methods', 'average (sum) pooling', 'max pooling', 'weighted-based pooling methods', 'feature pooling methods']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(8,11) :: feature pooling methods', '(12,15) :: scene character recognition', '(9,11) :: pooling methods', '(25,30) :: average (sum) pooling', '(31,33) :: max pooling', '(34,37) :: weighted-based pooling methods']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets; they are also easy to implement, and can be applied within various deep neural network architectures. These benefits come with only a light increase in computational overhead during training and a very modest increase in the number of model parameters.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['pooling operations']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['generalized pooling operation', 'average or max pooling', 'pooling operations', 'invariance properties', 'benchmark datasets', 'they', 'deep neural network architectures', 'computational overhead']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(4,7) :: generalized pooling operation', '(16,20) :: average or max pooling', '(27,29) :: pooling operations', '(33,35) :: invariance properties', '(50,52) :: benchmark datasets', '(53,54) :: they', '(66,70) :: deep neural network architectures', '(80,82) :: computational overhead']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "This way, a series of representations can be derived that trade-off generalization against specificity. Thereby we show a connection between NBNN classification and different pooling strategies, where, in contrast to traditional pooling schemes that perform spatial pooling of the features, pooling is performed in feature space. Moreover, rather than picking a single partitioning, we propose to combine them in a multi kernel framework.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['pooling schemes']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['representations', 'trade-off generalization', 'NBNN classification', 'pooling strategies', 'pooling schemes', 'spatial pooling of the features', 'pooling', 'feature space', 'them', 'multi kernel framework']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(6,7) :: representations', '(11,13) :: trade-off generalization', '(22,24) :: NBNN classification', '(26,28) :: pooling strategies', '(35,37) :: pooling schemes', '(39,44) :: spatial pooling of the features', '(26,27) :: pooling', '(49,51) :: feature space', '(65,66) :: them', '(68,71) :: multi kernel framework']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Tweet-pooling schemes were created based on mention/reply relationships between tweets and Twitter users, with several (non-networked) established methods also tested as a comparison. Results show that pooling tweets using network information gives better topic coherence and clustering performance than other pooling schemes, on the majority of datasets tested. Our findings contribute to an improved methodology for topic modeling with Twitter content.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['pooling schemes']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['Tweet-pooling schemes', 'mention/reply relationships', 'network information', 'topic coherence and clustering', 'pooling schemes', 'methodology', 'topic modeling', 'Twitter content']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,2) :: Tweet-pooling schemes', '(6,8) :: mention/reply relationships', '(33,35) :: network information', '(37,41) :: topic coherence and clustering', '(44,46) :: pooling schemes', '(60,61) :: methodology', '(62,64) :: topic modeling', '(65,67) :: Twitter content']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 523 recall: 47/59 = 0.797\n",
      "==============================================================================================================\n",
      "[4/30] Topic 524 — paragraphs: 37\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "In this paper we focus on 112 functions from the Python pandas library for DataFrame manipulation, an order of magnitude more than considered in prior approaches. To assess the viability of program synthesis in this domain, our first goal is a system that reliably synthesizes programs with a single library function. We introduce an encoding of structured input – output examples as graphs that can be fed to existing graph-based neural networks to infer the library function.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['program synthesis']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['Python pandas library', 'DataFrame manipulation', 'system', 'library function', 'structured input', 'graphs', 'graph-based neural networks', 'library function']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(10,13) :: Python pandas library', '(14,16) :: DataFrame manipulation', '(44,45) :: system', '(52,54) :: library function', '(60,62) :: structured input', '(66,67) :: graphs', '(73,76) :: graph-based neural networks']\n",
      "\n",
      "HITS=0  MISSES=1\n",
      "Missed gold (idx, span, text):\n",
      "  - #0 (33,34) :: program synthesis\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "Recent advances in program synthesis convinced us that it was the right time to transform the process of porting an operating system into a program synthesis problem. We set out to synthesize the needed machine dependent code for an existing operating system.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['program synthesis problem']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['program synthesis', 'operating system', 'program synthesis problem', 'machine dependent code', 'operating system']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(3,5) :: program synthesis', '(20,22) :: operating system', '(24,27) :: program synthesis problem', '(35,38) :: machine dependent code']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "The emphasis of the work has been on automating as much as possible of the program derivation process. Theorem-proving methods particularly well-suited to the program synthesis application have been developed. An interactive program-derivation system has been implemented.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['program synthesis application']\n",
      "\n",
      "[PRED mentions (strings)] n= 3\n",
      "['program derivation process', 'program synthesis application', 'interactive program-derivation system']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(15,18) :: program derivation process', '(25,28) :: program synthesis application', '(33,36) :: interactive program-derivation system']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "We show how the user of our system is involved in solving a program synthesis problem. We show that this interaction does not concern the problem of guiding the program synthesis process, this being solved by CM. Our experimental version serves to confirm that the system is worth to be developed.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['program synthesis process']\n",
      "\n",
      "[PRED mentions (strings)] n= 4\n",
      "['system', 'program synthesis problem', 'program synthesis process', 'system']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(7,8) :: system', '(13,16) :: program synthesis problem', '(30,33) :: program synthesis process']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 524 recall: 35/38 = 0.921\n",
      "==============================================================================================================\n",
      "[5/30] Topic 525 — paragraphs: 31\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "Thanks to the widespread use of digital pathology and the development of artificial intelligence methods, automatic histopathological image analysis methods help pathologists in their decision‐making process. In this process, rather than producing labels for whole‐slide image patches, semantic segmentation is very useful, which facilitates the pathologists ’ interpretation. In this study, automatic semantic segmentation based on cell type is proposed for the first time in the literature using novel deep convolutional networks structure (DCNN).\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['semantic segmentation']\n",
      "\n",
      "[PRED mentions (strings)] n= 6\n",
      "['digital pathology', 'artificial intelligence methods', 'automatic histopathological image analysis methods', 'semantic segmentation', 'automatic semantic segmentation', 'deep convolutional networks structure (DCNN']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(6,8) :: digital pathology', '(12,15) :: artificial intelligence methods', '(16,21) :: automatic histopathological image analysis methods', '(41,43) :: semantic segmentation', '(58,61) :: automatic semantic segmentation', '(76,82) :: deep convolutional networks structure (DCNN']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "This paper presents a semantic segmentation based method for automatically synthesizing two-tone cartoon portraits in black-and-white style. Synthesizing two-tone portraits from photographs can be considered as a heterogeneous image transformation problem, of which the result should be vivid portraits with distinct freehand-like features, such as clean backgrounds and continuous lines.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['semantic segmentation based method']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['semantic segmentation based method', 'automatically synthesizing two-tone cartoon portraits', 'black-and-white style', 'Synthesizing two-tone portraits', 'photographs', 'heterogeneous image transformation problem', 'freehand-like features', 'clean backgrounds', 'continuous lines']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(4,8) :: semantic segmentation based method', '(9,14) :: automatically synthesizing two-tone cartoon portraits', '(15,17) :: black-and-white style', '(18,21) :: Synthesizing two-tone portraits', '(22,23) :: photographs', '(28,32) :: heterogeneous image transformation problem', '(43,45) :: freehand-like features', '(48,50) :: clean backgrounds', '(51,53) :: continuous lines']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "The system solves different tasks (semantic segmentation and object detections) in an opportunistic and distributed fashion but still allows communication between modules to improve their respective performances. We propose the use of the semantic space to improve specific out-of-the-box object detectors and an update model to take the evidence from different detection into account in the semantic segmentation process. Our proposal is evaluated with the KITTI dataset, on the object detection benchmark and on five different sequences manually annotated for the semantic segmentation task, demonstrating the efficacy of our approach.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['semantic segmentation process']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['system', 'tasks', 'semantic space', 'out-of-the-box object detectors', 'update model', 'semantic segmentation process', 'KITTI dataset', 'object detection benchmark', 'semantic segmentation task', 'approach']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(1,2) :: system', '(4,5) :: tasks', '(36,38) :: semantic space', '(41,44) :: out-of-the-box object detectors', '(46,48) :: update model', '(59,62) :: semantic segmentation process', '(69,71) :: KITTI dataset', '(74,77) :: object detection benchmark', '(86,89) :: semantic segmentation task', '(95,96) :: approach']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Degraded-Manuscript Binarization in Diverse Document Textures and Layouts using Deep Encoder-Decoder Networks. Handwritten document-image binarization is a semantic segmentation process to differentiate ink pixels from background pixels. It is one of the essential steps towards character recognition, writer identification, and script-style evolution analysis.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['semantic segmentation process']\n",
      "\n",
      "[PRED mentions (strings)] n= 11\n",
      "['Degraded-Manuscript Binarization', 'Diverse Document Textures and Layouts', 'Deep Encoder-Decoder Networks', 'Handwritten document-image binarization', 'semantic segmentation process', 'ink pixels', 'background pixels', 'It', 'character recognition', 'writer identification', 'script-style evolution analysis']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,2) :: Degraded-Manuscript Binarization', '(3,8) :: Diverse Document Textures and Layouts', '(9,12) :: Deep Encoder-Decoder Networks', '(13,16) :: Handwritten document-image binarization', '(18,21) :: semantic segmentation process', '(23,25) :: ink pixels', '(26,28) :: background pixels', '(29,30) :: It', '(37,39) :: character recognition', '(40,42) :: writer identification', '(44,47) :: script-style evolution analysis']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 525 recall: 28/33 = 0.848\n",
      "==============================================================================================================\n",
      "[6/30] Topic 526 — paragraphs: 43\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "This paper describes the need and rationale for developing a phases model for guiding alcohol-problem prevention research. A phased approach to prevention research is consistent with such models developed for other health areas including heart disease, cancer, and drug testing. Such a model in alcohol prevention research can provide a means for (1) locating how far research has progressed along a continuum from basic or pre-intervention research to full implementation of preventive action, (2) identifying gaps in research, and (3) determining the level of empirical proof which exists for one or more prevention strategies prior to widespread dissemination.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['cancer']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['phases model', 'alcohol-problem prevention research', 'phased approach', 'prevention research', 'models', 'heart disease', 'cancer', 'drug testing', 'model', 'alcohol prevention research']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(10,12) :: phases model', '(14,17) :: alcohol-problem prevention research', '(19,21) :: phased approach', '(15,17) :: prevention research', '(28,29) :: models', '(35,37) :: heart disease', '(38,39) :: cancer', '(41,43) :: drug testing', '(11,12) :: model', '(48,51) :: alcohol prevention research']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "Colorectal polyps are important precursors to colon cancer, a major health problem. Colon capsule endoscopy is a safe and minimally invasive examination procedure, in which the images of the intestine are obtained via digital cameras on board of a small capsule ingested by a patient.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['colon cancer']\n",
      "\n",
      "[PRED mentions (strings)] n= 4\n",
      "['Colorectal polyps', 'colon cancer', 'Colon capsule endoscopy', 'digital cameras']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,2) :: Colorectal polyps', '(6,8) :: colon cancer', '(14,17) :: Colon capsule endoscopy', '(36,38) :: digital cameras']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "IntroductionThis paper 's aim is to develop a data warehouse from the integration of the files of three Brazilian health information systems concerned with the production of ambulatory and hospital procedures for cancer care, and cancer mortality. These systems do not have a unique patient identification, which makes their integration difficult even within a single system.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['cancer mortality']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['data warehouse', 'Brazilian health information systems', 'ambulatory and hospital procedures', 'cancer care', 'cancer mortality', 'systems', 'patient identification', 'system']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(8,10) :: data warehouse', '(18,22) :: Brazilian health information systems', '(27,31) :: ambulatory and hospital procedures', '(32,34) :: cancer care', '(36,38) :: cancer mortality', '(21,22) :: systems', '(46,48) :: patient identification', '(58,59) :: system']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "At the same level of false positives, best previous method detected 72.6%. We also show that probes with differing binding affinity both hinder differential expression detection and introduce artifacts in cancer-healthy tissue comparison. ConclusionsDetection and removal of such probes should be a routine step in Affymetrix data preprocessing.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['cancer-healthy tissue comparison']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['false positives', 'method', 'binding affinity', 'differential expression detection', 'artifacts', 'cancer-healthy tissue comparison', 'ConclusionsDetection', 'probes', 'Affymetrix data preprocessing']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(5,7) :: false positives', '(10,11) :: method', '(22,24) :: binding affinity', '(26,29) :: differential expression detection', '(31,32) :: artifacts', '(33,36) :: cancer-healthy tissue comparison', '(37,38) :: ConclusionsDetection', '(19,20) :: probes', '(49,52) :: Affymetrix data preprocessing']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 526 recall: 41/43 = 0.953\n",
      "==============================================================================================================\n",
      "[7/30] Topic 527 — paragraphs: 61\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "Among such approaches, Restricted Boltzmann Machine (RBM) is one of the most used technique for this purpose. Here, we propose to enhance the RBM performance on image denoising by adding a posterior supervision before its final denoising step. To this purpose, we propose a simple but effective approach that performs a fine-tuning in the RBM model.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['image denoising']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['approaches', 'Restricted Boltzmann Machine (RBM)', 'RBM', 'image denoising', 'posterior supervision', 'denoising step', 'approach', 'fine-tuning', 'RBM model']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(2,3) :: approaches', '(4,10) :: Restricted Boltzmann Machine (RBM)', '(8,9) :: RBM', '(31,33) :: image denoising', '(36,38) :: posterior supervision', '(41,43) :: denoising step', '(54,55) :: approach', '(58,59) :: fine-tuning', '(61,63) :: RBM model']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "We introduce a novel aggregation method to efficiently perform image denoising. Preliminary filters are aggregated in a non-linear fashion, using a new metric of pixel proximity based on how the pool of filters reaches a consensus.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['image denoising']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['aggregation method', 'image denoising', 'Preliminary filters', 'metric of pixel proximity', 'filters']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(4,6) :: aggregation method', '(9,11) :: image denoising', '(12,14) :: Preliminary filters', '(24,28) :: metric of pixel proximity', '(13,14) :: filters']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "In this paper, we propose a novel principal bundle model and apply it to the image denoising problem. This model is based on the fact that the patch manifold admits canonical groups actions such as rotation.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['image denoising problem']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['principal bundle model', 'it', 'image denoising problem', 'model', 'patch manifold', 'canonical groups actions', 'rotation']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(8,11) :: principal bundle model', '(13,14) :: it', '(16,19) :: image denoising problem', '(10,11) :: model', '(29,31) :: patch manifold', '(32,35) :: canonical groups actions', '(37,38) :: rotation']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "We proposed algorithm fully based on wavelet transform the main function of wavelet transform is real and imaginary value s into orthonormal series. Proposed two algorithms first one is dual tree complex wavelet transform (DTCWT) and another one is dual tree complex wavelet transform with orthogonal shift property (DTCWT with OSP) both are used for image denoising process.wavelet transform mostly used for image denoising process. In this algorithms are novel method forimpulse noise reduction in images.image\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['image denoising process']\n",
      "\n",
      "[PRED mentions (strings)] n= 13\n",
      "['algorithm', 'wavelet transform', 'wavelet transform', 'orthonormal series', 'algorithms', 'dual tree complex wavelet transform (DTCWT)', 'dual tree complex wavelet transform', 'orthogonal shift property (DTCWT with OSP)', 'image denoising process', 'wavelet transform', 'image denoising process', 'algorithms']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(2,3) :: algorithm', '(6,8) :: wavelet transform', '(21,23) :: orthonormal series', '(26,27) :: algorithms', '(30,38) :: dual tree complex wavelet transform (DTCWT)', '(30,35) :: dual tree complex wavelet transform', '(48,56) :: orthogonal shift property (DTCWT with OSP)', '(60,63) :: image denoising process', '(77,78) :: method']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 527 recall: 52/67 = 0.776\n",
      "==============================================================================================================\n",
      "[8/30] Topic 528 — paragraphs: 34\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "We show how effectively string kernels suit spam filtering problem. On the other hand, data preprocessing is a vital part of text classification where the objective is to generate feature vectors usable by SVM kernels. We detail a feature mapping variant in TC that yields improved performance for the standard SVM in filtering task.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['text classification']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['string kernels', 'spam filtering problem', 'data preprocessing', 'text classification', 'feature vectors', 'SVM kernels', 'feature mapping variant', 'TC', 'SVM', 'filtering task']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(4,6) :: string kernels', '(7,10) :: spam filtering problem', '(16,18) :: data preprocessing', '(23,25) :: text classification', '(31,33) :: feature vectors', '(35,37) :: SVM kernels', '(41,44) :: feature mapping variant', '(45,46) :: TC', '(35,36) :: SVM', '(55,57) :: filtering task']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "Our results are competitive with the most established text classification methods and our approach presents certain advantages, such as a good trade off between accuracy and scalability. Facing the particular challenges of the text classification problem, we also propose a new text preprocessing method that simultaneously performs both feature and instance selection. This intuitive method is computationally efficient and, most importantly, our experimental results show that it produces outstanding results.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['text classification problem']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['text classification methods', 'approach', 'text classification problem', 'text preprocessing method', 'feature and instance selection', 'method', 'it']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(8,11) :: text classification methods', '(13,14) :: approach', '(35,38) :: text classification problem', '(44,47) :: text preprocessing method', '(51,55) :: feature and instance selection', '(46,47) :: method', '(72,73) :: it']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "This paper covers a text classification problem: the identification of the author of a text. It is necessary to find author of a text with given information from a set of candidates whose sample texts were provided.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['text classification problem']\n",
      "\n",
      "[PRED mentions (strings)] n= 1\n",
      "['text classification problem']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(4,7) :: text classification problem']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Existing Machine Learning techniques yield close to human performance on text-based classification tasks. However, the presence of multi-modal noise in chat data such as emoticons, slang, spelling mistakes, code-mixed data, etc. makes existing deep-learning solutions perform poorly.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['text-based classification tasks']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['Existing Machine Learning techniques', 'text-based classification tasks', 'multi-modal noise', 'chat data', 'emoticons', 'slang', 'spelling mistakes', 'code-mixed data', 'deep-learning solutions']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,4) :: Existing Machine Learning techniques', '(10,13) :: text-based classification tasks', '(19,21) :: multi-modal noise', '(22,24) :: chat data', '(26,27) :: emoticons', '(28,29) :: slang', '(30,32) :: spelling mistakes', '(33,35) :: code-mixed data', '(40,42) :: deep-learning solutions']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 528 recall: 28/35 = 0.800\n",
      "==============================================================================================================\n",
      "[9/30] Topic 529 — paragraphs: 50\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "The core of our solution is to infer the temporal regularity from the original unprocessed video, and use it as a temporal consistency guide to stabilize the processed sequence. We formally characterize the frequency properties of our technique, and demonstrate, in practice, its ability to stabilize a wide range of popular image processing techniques including enhancement and stylization of color and tone, intrinsic images, and depth estimation.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['depth estimation']\n",
      "\n",
      "[PRED mentions (strings)] n= 11\n",
      "['solution', 'temporal regularity', 'unprocessed video', 'it', 'temporal consistency guide', 'processed sequence', 'technique', 'image processing techniques', 'enhancement and stylization of color and tone', 'intrinsic images', 'depth estimation']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(4,5) :: solution', '(9,11) :: temporal regularity', '(14,16) :: unprocessed video', '(19,20) :: it', '(22,25) :: temporal consistency guide', '(28,30) :: processed sequence', '(39,40) :: technique', '(56,59) :: image processing techniques', '(60,67) :: enhancement and stylization of color and tone', '(68,70) :: intrinsic images', '(72,74) :: depth estimation']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "The method can be generalized to arbitrary scenes, rather than focusing on a particular class of object. The synthesized light field can be used for various applications, such as depth estimation and refocusing.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['depth estimation']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['method', 'synthesized light field', 'applications', 'depth estimation', 'refocusing']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(1,2) :: method', '(20,23) :: synthesized light field', '(28,29) :: applications', '(32,34) :: depth estimation', '(35,36) :: refocusing']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "All these factors contribute to difficulty in accurate depth estimation. In this paper, we review five papers that attempt to solve the depth estimation problem with various techniques including supervised, weakly-supervised, and unsupervised learning techniques. We then compare these papers and understand the improvements made over one another.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['depth estimation problem']\n",
      "\n",
      "[PRED mentions (strings)] n= 4\n",
      "['depth estimation', 'depth estimation problem', 'techniques', 'supervised, weakly-supervised, and unsupervised learning techniques']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(8,10) :: depth estimation', '(24,27) :: depth estimation problem', '(29,30) :: techniques', '(31,39) :: supervised, weakly-supervised, and unsupervised learning techniques']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Our approach combines the Fourier decomposition of the Photography operator with a generalization of the discrete Radon transform and provides a fast and exact computation of the focal stack. We also generalize our formulation to compute the modified laplacian stack that can be used to solve the depth estimation problem. Finally we provide some computational results that show the validity of the approach.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['depth estimation problem']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['approach', 'Fourier decomposition of the Photography operator', 'discrete Radon transform', 'focal stack', 'formulation', 'modified laplacian stack', 'depth estimation problem', 'approach']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(1,2) :: approach', '(4,10) :: Fourier decomposition of the Photography operator', '(15,18) :: discrete Radon transform', '(27,29) :: focal stack', '(34,35) :: formulation', '(38,41) :: modified laplacian stack', '(48,51) :: depth estimation problem']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 529 recall: 43/51 = 0.843\n",
      "==============================================================================================================\n",
      "[10/30] Topic 530 — paragraphs: 67\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "Genetic algorithms have been successfully applied to a wide variety of problems. Although widely used, there are still some shortages.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['Genetic algorithms']\n",
      "\n",
      "[PRED mentions (strings)] n= 1\n",
      "['Genetic algorithms']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,2) :: Genetic algorithms']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "This article addresses the question whether or not the descriptive accuracy of the DARA algorithm benefits from the feature construction process. This involves solving the problem of constructing a relevant set of features for the DARA algorithm by using a genetic‐based algorithm. This work also evaluates several scoring measures used as fitness functions to find the best set of constructed features.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['genetic‐based algorithm']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['descriptive accuracy', 'DARA algorithm', 'feature construction process', 'features', 'DARA algorithm', 'geneticbased algorithm', 'scoring measures', 'fitness functions']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(9,11) :: descriptive accuracy', '(13,15) :: DARA algorithm', '(18,21) :: feature construction process', '(33,34) :: features', '(49,51) :: scoring measures', '(53,55) :: fitness functions']\n",
      "\n",
      "HITS=0  MISSES=1\n",
      "Missed gold (idx, span, text):\n",
      "  - #0 (41,42) :: genetic‐based algorithm\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "In this study, a novel fast genetic algorithm was proposed for constrained multi-objective optimization problems. The handling of constraint conditions were distributed to the initial population generation and each genetic process. Combine the constraint conditions and objectives, a new partial-order relation was introduced for comparison of individuals.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['genetic process']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['fast genetic algorithm', 'constrained multi-objective optimization problems', 'constraint conditions', 'population generation', 'genetic process', 'constraint conditions', 'partial-order relation']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(6,9) :: fast genetic algorithm', '(12,16) :: constrained multi-objective optimization problems', '(20,22) :: constraint conditions', '(27,29) :: population generation', '(31,33) :: genetic process', '(43,45) :: partial-order relation']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "It is well known that a judicious choice of crossover and/or mutation rates is critical to the success of genetic algorithms. Most earlier researches focused on finding optimal crossover or mutation rates, which vary for different problems, and even for different stages of the genetic process in a problem. In this paper, a generic scheme for adapting the crossover and mutation probabilities is proposed.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['genetic process']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['crossover and/or mutation rates', 'genetic algorithms', 'genetic process', 'generic scheme', 'crossover and mutation probabilities']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(9,13) :: crossover and/or mutation rates', '(19,21) :: genetic algorithms', '(47,49) :: genetic process', '(58,60) :: generic scheme', '(63,67) :: crossover and mutation probabilities']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 530 recall: 50/68 = 0.735\n",
      "==============================================================================================================\n",
      "[11/30] Topic 531 — paragraphs: 37\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "In order to learn the long-term dependencies in plant growth from the images, we propose to employ a Convolutional LSTM based framework. Especially, We apply an encoder-decoder model inspired by a framework on future frame prediction to model the representation of plant growth effectively. In addition, we propose two additional loss terms to put the constraints on shape changes of leaves between consecutive images.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['future frame prediction']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['plant growth', 'Convolutional LSTM based framework', 'encoder-decoder model', 'frame prediction', 'plant growth']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(8,10) :: plant growth', '(19,23) :: Convolutional LSTM based framework', '(29,31) :: encoder-decoder model', '(37,39) :: frame prediction']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "Moreover, if the predicted future is too short, it may not be fully usable by a human or other system. In this paper, we propose a novel method for future video prediction capable of generating multiple long-term futures. This makes the predictions more suitable for real applications.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['future video prediction']\n",
      "\n",
      "[PRED mentions (strings)] n= 2\n",
      "['method', 'future video prediction']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(31,32) :: method', '(33,36) :: future video prediction']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "The disentangled representation can be used for a range of tasks. For example, applying a standard LSTM to the time-vary components enables prediction of future frames. We evaluate our approach on a range of synthetic and real videos, demonstrating the ability to coherently generate hundreds of steps into the future.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['prediction of future frames']\n",
      "\n",
      "[PRED mentions (strings)] n= 6\n",
      "['disentangled representation', 'tasks', 'LSTM', 'time-vary components', 'approach', 'synthetic and real videos']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(1,3) :: disentangled representation', '(10,11) :: tasks', '(18,19) :: LSTM', '(21,23) :: time-vary components', '(32,33) :: approach', '(37,41) :: synthetic and real videos']\n",
      "\n",
      "HITS=0  MISSES=1\n",
      "Missed gold (idx, span, text):\n",
      "  - #0 (24,27) :: prediction of future frames\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "For this purpose, a Deep Convolutional Neural Network (CNN) topology is trained with an Adversarial loss in order to produce realistic future frames of the input image sequences. The prediction of future frames is incurred by two different approaches. In the first approach, a network is receiving 4 frames and predicting the next frame and in the second approach, given a sequence of 8 input images a network is predicting the next 8 frames.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['prediction of future frames']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['Deep Convolutional Neural Network ( CNN) topology', 'Adversarial loss', 'realistic future frames', 'image sequences', 'approaches', 'approach', 'network', 'network']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(17,19) :: Adversarial loss', '(23,26) :: realistic future frames', '(29,31) :: image sequences', '(42,43) :: approaches', '(47,48) :: approach', '(8,9) :: network']\n",
      "\n",
      "HITS=0  MISSES=1\n",
      "Missed gold (idx, span, text):\n",
      "  - #0 (33,36) :: prediction of future frames\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 531 recall: 30/37 = 0.811\n",
      "==============================================================================================================\n",
      "[12/30] Topic 532 — paragraphs: 65\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "This paper describes a general framework for transforming a sequential program into a network of processes, which are then converted to hardware accelerators through high level synthesis. Also proposed is a complementing technique for performing static deadlock analysis of the generated accelerator network.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['high level synthesis']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['framework', 'sequential program', 'network of processes', 'hardware accelerators', 'high level synthesis', 'complementing technique', 'static deadlock analysis of the generated accelerator network']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(5,6) :: framework', '(9,11) :: sequential program', '(13,16) :: network of processes', '(22,24) :: hardware accelerators', '(25,28) :: high level synthesis', '(33,35) :: complementing technique', '(37,45) :: static deadlock analysis of the generated accelerator network']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "FPGAs have been rapidly adopted for acceleration of Deep Neural Networks (DNNs) with improved latency and energy efficiency compared to CPU and GPU-based implementations. High-level synthesis (HLS) is an effective design flow for DNNs due to improved productivity, debugging, and design space exploration ability. However, optimizing large neural networks under resource constraints for FPGAs is still a key challenge.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['High-level synthesis']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['FPGAs', 'acceleration of Deep Neural Networks ( DNNs)', 'latency and energy efficiency', 'CPU and GPU-based implementations', 'High-level synthesis (HLS)', 'DNNs', 'resource constraints', 'FPGAs']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,1) :: FPGAs', '(16,20) :: latency and energy efficiency', '(22,26) :: CPU and GPU-based implementations', '(27,32) :: High-level synthesis (HLS)', '(12,13) :: DNNs', '(59,61) :: resource constraints']\n",
      "\n",
      "HITS=0  MISSES=1\n",
      "Missed gold (idx, span, text):\n",
      "  - #0 (27,28) :: High-level synthesis\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "In this paper, we accelerate the proposed algorithm with simultaneous reconstruction and segmentation using the Mumford-Shah model by FPGA devices. The algorithm is hand-optimized with both algorithmic domain knowledge and platform-specific information before translated into FPGA implementation using high-level synthesis and other electronic system-level design tools. A high-level performance model is used to guide the design and optimization process at early stages.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['high-level synthesis']\n",
      "\n",
      "[PRED mentions (strings)] n= 12\n",
      "['algorithm', 'simultaneous reconstruction and segmentation', 'Mumford-Shah model', 'FPGA devices', 'algorithm', 'algorithmic domain knowledge', 'platform-specific information', 'FPGA implementation', 'high-level synthesis', 'electronic system-level design tools', 'high-level performance model', 'design and optimization process']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(8,9) :: algorithm', '(10,14) :: simultaneous reconstruction and segmentation', '(16,18) :: Mumford-Shah model', '(19,21) :: FPGA devices', '(28,31) :: algorithmic domain knowledge', '(32,34) :: platform-specific information', '(37,39) :: FPGA implementation', '(40,42) :: high-level synthesis', '(44,48) :: electronic system-level design tools', '(50,53) :: high-level performance model', '(58,62) :: design and optimization process']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Power consumption could easily be computed as the sum of the power consumptions of individual chips, and hot chips could be cooled with a heatsink. Partitioning onto multiple boards was performed manually, or at least handled separately from the high-level synthesis process. General-purpose multiprocessors were constructed only in research laboratories and special-purpose multiprocessors were expensive and constructed for only a few applications.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['high-level synthesis process']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['Power consumption', 'power consumptions', 'heatsink', 'boards', 'high-level synthesis process', 'General-purpose multiprocessors', 'special-purpose multiprocessors']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,2) :: Power consumption', '(11,13) :: power consumptions', '(25,26) :: heatsink', '(30,31) :: boards', '(42,45) :: high-level synthesis process', '(46,48) :: General-purpose multiprocessors', '(55,57) :: special-purpose multiprocessors']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 532 recall: 63/68 = 0.926\n",
      "==============================================================================================================\n",
      "[13/30] Topic 533 — paragraphs: 46\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "Recent works have demonstrated that the convolutional descriptor aggregation can provide state-of-the-art performance for image retrieval. In this paper, we propose a multi-center convolutional descriptor aggregation (MCDA) method to produce global image representation for image retrieval. We first present a feature map center selection method to eliminate the background information in the feature maps.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['image retrieval']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['convolutional descriptor aggregation', 'image retrieval', 'multi-center convolutional descriptor aggregation (MCDA) method', 'global image representation', 'image retrieval', 'feature map center selection method', 'feature maps']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(6,9) :: convolutional descriptor aggregation', '(14,16) :: image retrieval', '(24,32) :: multi-center convolutional descriptor aggregation (MCDA) method', '(34,37) :: global image representation', '(45,50) :: feature map center selection method', '(57,59) :: feature maps']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "Frequency features are described as the generalized Gaussian density (GGD) of Contourlet transform detail coefficients and LBP histogram of approximation coefficients. Further, we use closed-loop feedback to adjust weighting factor adoptively for image retrieval. Experiments show that average recall rate of this method is 12.08%, 10.23% higher than frequency domain method and LBP respectively.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['image retrieval']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['Frequency features', 'generalized Gaussian density (GGD) of Contourlet transform detail coefficients', 'LBP histogram', 'closed-loop feedback', 'weighting factor', 'image retrieval', 'average recall rate', 'method', 'frequency domain method', 'LBP']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,2) :: Frequency features', '(6,17) :: generalized Gaussian density (GGD) of Contourlet transform detail coefficients', '(18,20) :: LBP histogram', '(28,30) :: closed-loop feedback', '(32,34) :: weighting factor', '(36,38) :: image retrieval', '(42,45) :: average recall rate', '(47,48) :: method', '(56,59) :: frequency domain method', '(18,19) :: LBP']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "Information technology and Internet use and popularity of various digital terminal equipment, greatly enrich the database of digital images, at the same time, also brought new challenges to the image information retrieval. Content-based image retrieval technology is the key content in the field of image retrieval theory research, this article embarks from the content-based image retrieval technology system, studies the retrieval technology of the composition of the system framework and module, and expounds in detail the implementation process of content-based image retrieval technology.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['image information retrieval']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['digital terminal equipment', 'database of digital images', 'image information retrieval', 'Content-based image retrieval technology', 'image retrieval theory research', 'content-based image retrieval technology system', 'retrieval technology', 'system framework', 'module', 'content-based image retrieval technology']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(9,12) :: digital terminal equipment', '(16,20) :: database of digital images', '(32,35) :: image information retrieval', '(36,40) :: Content-based image retrieval technology', '(48,52) :: image retrieval theory research', '(58,63) :: content-based image retrieval technology system', '(38,40) :: retrieval technology', '(73,75) :: system framework', '(76,77) :: module']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Among the above features, activity distribution (in finding scars if any) was found to have the least interobserver reproducibility. Therefore, in this study, we developed an image-based retrieval (IBR) and a computer-based diagnosis (CAD) system, focused on this feature in particular. The developed IBR and CAD algorithms start with automatic segmentation, boundary and landmark detection.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['image-based retrieval']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['features', 'activity distribution (in finding scars', 'image-based retrieval (IBR)', 'computer-based diagnosis (CAD) system', 'IBR and CAD algorithms', 'automatic segmentation', 'boundary and landmark detection']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(3,4) :: features', '(5,11) :: activity distribution (in finding scars', '(32,37) :: image-based retrieval (IBR)', '(39,45) :: computer-based diagnosis (CAD) system', '(55,59) :: IBR and CAD algorithms', '(61,63) :: automatic segmentation', '(64,68) :: boundary and landmark detection']\n",
      "\n",
      "HITS=0  MISSES=1\n",
      "Missed gold (idx, span, text):\n",
      "  - #0 (32,33) :: image-based retrieval\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 533 recall: 38/46 = 0.826\n",
      "==============================================================================================================\n",
      "[14/30] Topic 534 — paragraphs: 32\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "The fashion of generating masks leads to different learning methods, i.e., DropOut, DropConnect. We propose a new method called DropPart which is a generalization of DropConnect. In DropPart the Beta distribution instead of Bernoulli distribution in DropConnect is used.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['DropConnect']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['generating masks', 'learning methods', 'DropOut', 'DropConnect', 'method', 'DropPart', 'DropConnect', 'DropPart the Beta distribution', 'DropConnect']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(3,5) :: generating masks', '(8,10) :: learning methods', '(13,14) :: DropOut', '(15,16) :: DropConnect', '(21,22) :: method', '(23,24) :: DropPart', '(32,36) :: DropPart the Beta distribution']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "While training with the two regularization methods, a randomly chosen subsets of activations/weights are dropped. Consequently, the assessment on the HACDB database to treat character level proves shows an improvement of classification error rate once adding Dropout and DropConnect techniques.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['Dropout and DropConnect techniques']\n",
      "\n",
      "[PRED mentions (strings)] n= 6\n",
      "['regularization methods', 'randomly chosen subsets of activations/weights', 'HACDB database', 'character level', 'classification error rate', 'Dropout and DropConnect techniques']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(5,7) :: regularization methods', '(9,14) :: randomly chosen subsets of activations/weights', '(23,25) :: HACDB database', '(27,29) :: character level', '(34,37) :: classification error rate', '(39,43) :: Dropout and DropConnect techniques']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "In this work, we leverage advances in sparse coding techniques to reduce the number of trainable parameters in a fully connected neural network. While most of the works in literature impose $ \\ell_1 $ regularization, DropOut or DropConnect techniques to induce sparsity, our scheme considers feature importance as a criterion to allocate the trainable parameters (resources) efficiently in the network. Even though sparsity is ensured, $ \\ell_1 $ regularization requires training on all the resources in a deep neural network.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['DropConnect techniques']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['sparse coding techniques', 'fully connected neural network', 'sparsity', 'scheme', 'feature importance', 'network', 'deep neural network']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(8,11) :: sparse coding techniques', '(20,24) :: fully connected neural network', '(44,45) :: sparsity', '(47,48) :: scheme', '(49,51) :: feature importance', '(23,24) :: network', '(85,88) :: deep neural network']\n",
      "\n",
      "HITS=0  MISSES=1\n",
      "Missed gold (idx, span, text):\n",
      "  - #0 (40,41) :: DropConnect techniques\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "We assign the basic unit of drop method for convolutional weights to be the whole kernel windows, so one output map value is dropped. We evaluated the proposed DropKernel strategy by the object classification performance on CIFAR10 in comparison to conventional Dropout and DropConnect methods, and showed improved performance of the proposed method.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['Dropout and DropConnect methods']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['unit of drop method', 'convolutional weights', 'kernel windows', 'DropKernel strategy', 'object classification', 'CIFAR10', 'Dropout and DropConnect methods', 'method']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(4,8) :: unit of drop method', '(9,11) :: convolutional weights', '(15,17) :: kernel windows', '(30,32) :: DropKernel strategy', '(34,36) :: object classification', '(38,39) :: CIFAR10', '(43,47) :: Dropout and DropConnect methods', '(7,8) :: method']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 534 recall: 26/33 = 0.788\n",
      "==============================================================================================================\n",
      "[15/30] Topic 535 — paragraphs: 60\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "A decision tree is an important means of data mining and inductive learning, which is usually used to form classifiers and prediction models. C4.5 is one of the most classic classification algorithms on data mining, but when it is used in mass calculations, the efficiency is very low.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['data mining']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['decision tree', 'data mining', 'inductive learning', 'classifiers', 'prediction models', 'C4.5', 'classification algorithms', 'data mining', 'it', 'mass calculations']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(1,3) :: decision tree', '(8,10) :: data mining', '(11,13) :: inductive learning', '(20,21) :: classifiers', '(22,24) :: prediction models', '(25,26) :: C4.5', '(32,34) :: classification algorithms', '(40,41) :: it', '(44,46) :: mass calculations']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "This book offers an overview of knowledge management. It starts with an introduction to the subject, placing descriptive models in the context of the overall field as well as within the more specific field of data mining analysis. Chapter 2 covers data visualization, including directions for accessing R open source software (described through Rattle).\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['data mining analysis']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['knowledge management', 'descriptive models', 'data mining analysis', 'data visualization', 'R open source software']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(6,8) :: knowledge management', '(19,21) :: descriptive models', '(37,40) :: data mining analysis', '(44,46) :: data visualization', '(51,55) :: R open source software']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "The data were extracted from the information system of the healthcare network. We performed conventional statistical analysis and data mining analysis using mainly Bayesian networks. The Bayesian model showed that the probability of registration on the waiting list is associated to age, cardiovascular disease, diabetes, serum albumin level, respiratory disease, physical impairment, follow-up in the department performing transplantation and past history of malignancy.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['data mining analysis']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['statistical analysis', 'data mining analysis', 'Bayesian networks', 'Bayesian model', 'waiting list', 'cardiovascular disease', 'serum albumin level', 'physical impairment', 'transplantation', 'malignancy']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(16,18) :: statistical analysis', '(19,22) :: data mining analysis', '(24,26) :: Bayesian networks', '(28,30) :: Bayesian model', '(38,40) :: waiting list', '(45,47) :: cardiovascular disease', '(50,53) :: serum albumin level', '(57,59) :: physical impairment', '(65,66) :: transplantation', '(70,71) :: malignancy']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Data mining technology is gradually integrated into the decision support system, so to solveproblem of knowledge acquisition \" bottleneck \" facing by the traditional decision support system and other problems in series, and becomes an important part of the decision-making system. Data Mining Data mining also known as dada exploit and data excavation is an advanced process of extracting model (knowledge) that is potential, effective and easy for understanding according to set business object by using methods based on computer (including new technologies). Prediction and description are two basic goals of the data mining.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['Data Mining Data mining']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['Data mining technology', 'decision support system', 'knowledge acquisition', 'decision support system', 'decision-making system', 'Data Mining Data mining', 'data mining']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,3) :: Data mining technology', '(8,11) :: decision support system', '(16,18) :: knowledge acquisition', '(41,43) :: decision-making system', '(44,48) :: Data Mining Data mining', '(0,2) :: data mining']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 535 recall: 53/60 = 0.883\n",
      "==============================================================================================================\n",
      "[16/30] Topic 536 — paragraphs: 63\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "The research activities are based on the extension of the IMS platform to support a common management architecture that supports heterogeneous networks and the use of 802.21 protocol for transparent user switching. To sum up, the individual results of this study can be used for the design, implementation and evaluation of a framework for secure and optimized multimedia services taking into account: - Security issues related to IP audio and video services. - Methods of detection of security problems.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['Security issues']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['IMS platform', 'common management architecture', 'heterogeneous networks', 'framework', 'multimedia services', 'Security issues', 'IP audio and video services', 'detection of security problems']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(10,12) :: IMS platform', '(15,18) :: common management architecture', '(20,22) :: heterogeneous networks', '(55,56) :: framework', '(60,62) :: multimedia services', '(67,69) :: Security issues', '(71,76) :: IP audio and video services', '(80,84) :: detection of security problems']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "Cloud computing provides the way to share distributed resources and services that belong to different organizations or sites. Since Cloud computing share distributed resources via network in the open environment thus it makes security problems. In contrast to traditional solutions, where the IT services are under proper physical, logical and personnel controls, Cloud Computing moves the application software and databases to the large data centres, where the management of the data and services may not be fully trustworthy.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['security problems']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['Cloud computing', 'distributed resources', 'Cloud computing', 'distributed resources', 'security problems', 'IT services', 'physical, logical and personnel controls', 'Cloud Computing', 'application software']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,2) :: Cloud computing', '(7,9) :: distributed resources', '(34,36) :: security problems', '(45,47) :: IT services', '(50,56) :: physical, logical and personnel controls', '(61,63) :: application software']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "For a small home or office environment the currently available infrastructure might be adequate, but for mission-critical applications it lacks essential security properties. In the sequel we identify weak points in the Jini architecture and its protocols and propose an extension to the architecture that provides a solution to the identified security problems. We describe the design choices underlying our implementation which aims at maximum compatibility with the existing Jini specifications.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['security problems']\n",
      "\n",
      "[PRED mentions (strings)] n= 6\n",
      "['mission-critical applications', 'Jini architecture', 'extension', 'security problems', 'maximum compatibility', 'Jini specifications']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(17,19) :: mission-critical applications', '(34,36) :: Jini architecture', '(42,43) :: extension', '(53,55) :: security problems', '(67,69) :: maximum compatibility', '(72,74) :: Jini specifications']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Security concerns for battery-operated wireless systems require the development of energy-efficient data-encryption techniques that can adapt to the time-varying data rates and quality-of-service requirements inherent in a wireless application. This work describes the design and implementation of a configurable encryption processor that allows the security provided to be traded off with respect to the energy that is dissipated to encrypt a bit.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['Security concerns']\n",
      "\n",
      "[PRED mentions (strings)] n= 6\n",
      "['Security concerns', 'battery-operated wireless systems', 'energy-efficient data-encryption techniques', 'wireless application', 'encryption processor', 'security']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,2) :: Security concerns', '(3,6) :: battery-operated wireless systems', '(10,13) :: energy-efficient data-encryption techniques', '(27,29) :: wireless application', '(40,42) :: encryption processor', '(0,1) :: security']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 536 recall: 55/63 = 0.873\n",
      "==============================================================================================================\n",
      "[17/30] Topic 537 — paragraphs: 52\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "Services is a suite of Web services for customers with integration and business-to-business collaboration requirements. These services include workflow, access control, and service bus connectivity — enabling developers to extend applications to the cloud through a flexible, scalable and interoperable platform.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['access control']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['Services', 'Web services', 'integration and business-to-business collaboration requirements', 'services', 'workflow', 'access control', 'service bus connectivity']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,1) :: Services', '(5,7) :: Web services', '(10,15) :: integration and business-to-business collaboration requirements', '(19,20) :: workflow', '(21,23) :: access control', '(25,28) :: service bus connectivity']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "The notion of attribute-based encryption (ABE) was proposed as an economical alternative to public-key infrastructures. It is the set of descriptive attributes, used as an identity to generate a secret key, as well as serving as the access structure that performs access control. ABE is also a useful building block in various cryptographic primitives such as searchable encryption..\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['access control']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['attribute-based encryption ( ABE)', 'public-key infrastructures', 'It', 'descriptive attributes', 'access control', 'ABE', 'cryptographic primitives', 'searchable encryption']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(15,17) :: public-key infrastructures', '(18,19) :: It', '(23,25) :: descriptive attributes', '(46,48) :: access control', '(6,7) :: ABE', '(58,60) :: cryptographic primitives', '(62,64) :: searchable encryption']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "Access control is essential to computer security, especially in an open, distributed, networked communication environment. Modern access control model such as UCON aims at accommodating general requirements.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['Access control']\n",
      "\n",
      "[PRED mentions (strings)] n= 4\n",
      "['Access control', 'computer security', 'access control model', 'UCON']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,2) :: Access control', '(5,7) :: computer security', '(20,23) :: access control model', '(25,26) :: UCON']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Security services in a multi-user environment are often based on access control mechanisms. Static aspects of an access control policy can be formalised using abstract algebraic models. We integrate these static aspects into a dynamic framework considering requesting access to resources as a process aiming at the prevention of access control violations when a program is executed.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['access control policy']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['Security services', 'access control mechanisms', 'Static aspects', 'access control policy', 'abstract algebraic models', 'static aspects', 'dynamic framework', 'access control violations']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,2) :: Security services', '(10,13) :: access control mechanisms', '(14,16) :: Static aspects', '(18,21) :: access control policy', '(25,28) :: abstract algebraic models', '(36,38) :: dynamic framework', '(51,54) :: access control violations']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 537 recall: 48/54 = 0.889\n",
      "==============================================================================================================\n",
      "[18/30] Topic 538 — paragraphs: 44\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "Spoken language understanding (SLU) is a key component of task-oriented dialogue systems. SLU parses natural language user utterances into semantic frames.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['task-oriented dialogue systems']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['Spoken language understanding (SLU)', 'task-oriented dialogue systems', 'SLU', 'natural language user utterances', 'semantic frames']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,6) :: Spoken language understanding (SLU)', '(11,14) :: task-oriented dialogue systems', '(4,5) :: SLU', '(17,21) :: natural language user utterances', '(22,24) :: semantic frames']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "We describe a two-step approach for dialogue management in task-oriented spoken dialogue systems. A unified neural network framework is proposed to enable the system to first learn by supervision from a set of dialogue data and then continuously improve its behaviour via reinforcement learning, all using gradient-based algorithms on one single model.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['task-oriented spoken dialogue systems']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['two-step approach', 'dialogue management', 'task-oriented spoken dialogue systems', 'unified neural network framework', 'system', 'dialogue data', 'reinforcement learning', 'gradient-based algorithms', 'model']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(3,5) :: two-step approach', '(6,8) :: dialogue management', '(9,13) :: task-oriented spoken dialogue systems', '(15,19) :: unified neural network framework', '(24,25) :: system', '(34,36) :: dialogue data', '(43,45) :: reinforcement learning', '(48,50) :: gradient-based algorithms', '(53,54) :: model']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "We describe a framework for deep linguistic processing for natural language understanding in task-oriented spoken dialogue systems. The goal is to create domaingeneral processing techniques that can be shared across all domains and dialogue tasks, combined with domain-specific optimization based on an ontology mapping from the generic LF to the application ontology.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['task-oriented spoken dialogue systems']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['framework', 'deep linguistic processing', 'natural language understanding', 'task-oriented spoken dialogue systems', 'domaingeneral processing techniques', 'dialogue tasks', 'domain-specific optimization', 'ontology mapping', 'generic LF', 'ontology']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(3,4) :: framework', '(5,8) :: deep linguistic processing', '(9,12) :: natural language understanding', '(13,17) :: task-oriented spoken dialogue systems', '(23,26) :: domaingeneral processing techniques', '(34,36) :: dialogue tasks', '(39,41) :: domain-specific optimization', '(44,46) :: ontology mapping', '(48,50) :: generic LF', '(44,45) :: ontology']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "To design a task-independent dialogue system, we present a task-oriented dialogue analysis in terms of finding the referents of definite descriptions and we show how this analysis leads to a goal-oriented inferential representation of the task. This representation provides a logical generic model of the task, which is compatible with a belief system.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['task-oriented dialogue analysis']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['task-independent dialogue system', 'task-oriented dialogue analysis', 'analysis', 'goal-oriented inferential representation of the task', 'representation', 'logical generic model', 'task', 'belief system']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(3,6) :: task-independent dialogue system', '(10,13) :: task-oriented dialogue analysis', '(12,13) :: analysis', '(31,37) :: goal-oriented inferential representation of the task', '(33,34) :: representation', '(42,45) :: logical generic model', '(36,37) :: task', '(54,56) :: belief system']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 538 recall: 41/45 = 0.911\n",
      "==============================================================================================================\n",
      "[19/30] Topic 539 — paragraphs: 61\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "To address this problem it is important to know how users interact with the system. This work constitutes a methodological contribution capable of identifying the context of use in which users perform interactions with a groupware application (synchronous or asynchronous) and provides, using machine learning techniques, generative models of how users behave. Additionally, these models are transformed into a text that describes in natural language the main characteristics of the interaction of the users with the system.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['generative models']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['system', 'methodological contribution', 'groupware application', 'machine learning techniques', 'generative models', 'models', 'system']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(14,15) :: system', '(20,22) :: methodological contribution', '(36,38) :: groupware application', '(47,50) :: machine learning techniques', '(51,53) :: generative models', '(52,53) :: models']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "The multimedia event is directly related to a series of works by Boetti featuring a grid-filling algorithm which has been implemented by the artist on a huge variety of planar supports through many collective projects. In this sense, the event is intended to be another possible realization of the original work exploiting a multimodal support, i.e. including sound (music and audio processing) and emphasizing the generative process defined by Boetti 's algorithm through interactivity. An analysis of a corpus of 50 realized Boetti 's works - all based on the same algorithm - has been conducted in order to define meaningful mapping strategies from the visual domain to music and audio-video processing.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['generative process']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['multimedia event', 'grid-filling algorithm', 'multimodal support', 'sound (music and audio processing)', 'generative process', \"Boetti 's algorithm\", \"Boetti 's works\", 'algorithm', 'visual domain', 'music and audio-video processing']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(1,3) :: multimedia event', '(15,17) :: grid-filling algorithm', '(55,57) :: multimodal support', '(60,67) :: sound (music and audio processing)', '(70,72) :: generative process', \"(74,77) :: Boetti 's algorithm\", \"(88,91) :: Boetti 's works\", '(16,17) :: algorithm', '(111,113) :: visual domain', '(114,118) :: music and audio-video processing']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "When the MAR assumption is incorrect, inferences are biased and predictive performance can suffer. Therefore, we model both the generative process for the data and the missing data mechanism. By learning these two models jointly we obtain improved performance over state-of-the-art methods when predicting the ratings and when modeling the data observation process.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['generative process']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['MAR assumption', 'inferences', 'generative process', 'models', 'data observation process']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(2,4) :: MAR assumption', '(7,8) :: inferences', '(22,24) :: generative process', '(37,38) :: models', '(54,57) :: data observation process']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Here, we instead view adversarial attacks as a generative modelling problem, with the goal of producing entire distributions of adversarial examples given an unperturbed input. We show that this generative perspective can be used to design a unified encoder-decoder framework, which is domain-agnostic in that the same framework can be employed to attack different domains with minimal modification. Across three diverse domains --- images, text, and graphs --- our approach generates whitebox attacks with success rates that are competitive with or superior to existing approaches, with a new state-of-the-art achieved in the graph domain.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['generative perspective']\n",
      "\n",
      "[PRED mentions (strings)] n= 13\n",
      "['adversarial attacks', 'generative modelling problem', 'generative perspective', 'unified encoder-decoder framework', 'framework', 'images', 'text', 'graphs', 'approach', 'whitebox attacks', 'success rates', 'approaches']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(5,7) :: adversarial attacks', '(9,12) :: generative modelling problem', '(32,34) :: generative perspective', '(40,43) :: unified encoder-decoder framework', '(42,43) :: framework', '(68,69) :: images', '(70,71) :: text', '(73,74) :: graphs', '(76,77) :: approach', '(78,80) :: whitebox attacks', '(81,83) :: success rates', '(91,92) :: approaches']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 539 recall: 50/64 = 0.781\n",
      "==============================================================================================================\n",
      "[20/30] Topic 540 — paragraphs: 59\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "This minimum MSE optimization is performed via the minimization of the so-called chi-square unbiased risk estimate (CURE). Taking advantage of some acceleration techniques involving convolutions and parallel computation, we show that the proposed CURE-optimized NLM outperforms some state-of-the-art NLM algorithms with no increase in computation time.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['convolutions']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['minimum MSE optimization', 'minimization of the so-called [ chi-square unbiased risk estimate ( CURE)', 'acceleration techniques', 'convolutions', 'parallel computation', 'CURE-optimized NLM', 'NLM algorithms', 'computation time']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(1,4) :: minimum MSE optimization', '(24,26) :: acceleration techniques', '(27,28) :: convolutions', '(29,31) :: parallel computation', '(37,39) :: CURE-optimized NLM', '(42,44) :: NLM algorithms', '(48,50) :: computation time']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "Convolutions are the fundamental building blocks of CNNs. The fact that their weights are spatially shared is one of the main reasons for their widespread use, but it is also a major limitation, as it makes convolutions content-agnostic. We propose a pixel-adaptive convolution (PAC) operation, a simple yet effective modification of standard convolutions, in which the filter weights are multiplied with a spatially varying kernel that depends on learnable, local pixel features.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['convolutions']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['Convolutions', 'CNNs', 'it', 'it', 'convolutions content-agnostic', 'pixel-adaptive convolution (PAC) operation', 'spatially varying kernel', 'local pixel features']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,1) :: Convolutions', '(7,8) :: CNNs', '(29,30) :: it', '(39,41) :: convolutions content-agnostic', '(45,51) :: pixel-adaptive convolution (PAC) operation', '(70,73) :: spatially varying kernel', '(78,81) :: local pixel features']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "In this work, we explore the benefits of generalizing one step further into the hyper-complex numbers, quaternions specifically, and provide the architecture components needed to build deep quaternion networks. We develop the theoretical basis by reviewing quaternion convolutions, developing a novel quaternion weight initialization scheme, and developing novel algorithms for quaternion batch-normalization. These pieces are tested in a classification model by end-to-end training on the CIFAR −10 and CIFAR −100 data sets and a segmentation model by end-to-end training on the KITTI Road Segmentation data set.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['quaternion convolutions']\n",
      "\n",
      "[PRED mentions (strings)] n= 11\n",
      "['hyper-complex numbers', 'quaternions', 'architecture components', 'deep quaternion networks', 'quaternion convolutions', 'quaternion weight initialization scheme', 'algorithms', 'quaternion batch-normalization', 'pieces', 'classification model', 'CIFAR 10 and CIFAR 100 data sets and a [ segmentation model']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(15,17) :: hyper-complex numbers', '(18,19) :: quaternions', '(24,26) :: architecture components', '(29,32) :: deep quaternion networks', '(40,42) :: quaternion convolutions', '(46,50) :: quaternion weight initialization scheme', '(54,55) :: algorithms', '(56,58) :: quaternion batch-normalization', '(60,61) :: pieces', '(65,67) :: classification model']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Different angles of rotation are a key features of the new filter such that horizontal, vertical, left, and right-diagonal LQS filter masks rotate pixels through angles π/2, 5π/2, 3π/2, and 7π/2, respectively. Although, the four LQS masks are combined parallel to make a single LQS mask but derived using four quaternion convolutions, one for each direction of edges, the LQS filter produces a result without the combination of results from four separate edge detectors. This methodology could be generalised to design more elaborate LQS filters to perform other geometric operations on colour image pixels.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['quaternion convolutions']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['filter', 'horizontal, vertical, left, and right-diagonal LQS filter masks', 'LQS masks', 'LQS mask', 'quaternion convolutions', 'LQS filter', 'edge detectors', 'methodology', 'LQS filters', 'geometric operations on colour image pixels']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(11,12) :: filter', '(14,25) :: horizontal, vertical, left, and right-diagonal LQS filter masks', '(44,46) :: LQS masks', '(53,55) :: LQS mask', '(59,61) :: quaternion convolutions', '(22,24) :: LQS filter', '(83,85) :: edge detectors', '(87,88) :: methodology', '(95,97) :: LQS filters', '(100,106) :: geometric operations on colour image pixels']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 540 recall: 47/62 = 0.758\n",
      "==============================================================================================================\n",
      "[21/30] Topic 541 — paragraphs: 34\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "Image quality assessment is a key issue in image processing research. The paper makes a comprehensive and systematic analysis of image quality assessment methods, such as the principles, effectiveness, advantages and disadvantages, and application scope.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['Image quality assessment']\n",
      "\n",
      "[PRED mentions (strings)] n= 3\n",
      "['Image quality assessment', 'image processing research', 'image quality assessment methods']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,3) :: Image quality assessment', '(8,11) :: image processing research', '(21,25) :: image quality assessment methods']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "However, to the best of our knowledge, no research has yet been done on the line of sight in the case of showing a video of different resolutions and sizes at the same viewing distance. This is different from the general conditions in image quality evaluation, but we often watch videos under this condition in daily life. Therefore, we measured mean gaze point position when screen resolution and size are changed, finding that, if resolution was changed, only different viewing positions affected the position of the mean gaze point.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['image quality evaluation']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['image quality evaluation', 'mean gaze point position', 'screen resolution', 'viewing positions', 'mean gaze point']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(46,49) :: image quality evaluation', '(66,70) :: mean gaze point position', '(71,73) :: screen resolution', '(88,90) :: viewing positions', '(66,69) :: mean gaze point']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "Generally, there are several ways how to assess image quality. Three main approaches are: subjective testing, objective testing and image quality evaluation using a human visual system model (HVS). The subjective testing is based on human perception, the objective testing on a mathematical computing and the human vision models on mathematical modelling of the human vision with respecting the human perception properties.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['image quality evaluation']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['image quality', 'subjective testing', 'objective testing', 'image quality evaluation', 'human visual system model ( HVS)', 'human perception', 'mathematical computing', 'human vision models', 'mathematical modelling of the human vision', 'human perception properties']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(9,11) :: image quality', '(17,19) :: subjective testing', '(20,22) :: objective testing', '(23,26) :: image quality evaluation', '(42,44) :: human perception', '(50,52) :: mathematical computing', '(54,57) :: human vision models', '(58,64) :: mathematical modelling of the human vision', '(67,70) :: human perception properties']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "The collected data allow (1) to determine the best set of parameters to use for this image-based quality assessment approach and (2) to compare this approach to the best performing model-based metrics and determine for which use-case they are respectively adapted. We conclude by exploring several applications that illustrate the benefits of image-based quality assessment.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['image-based quality assessment']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['image-based quality assessment approach', 'approach', 'model-based metrics', 'applications', 'image-based quality assessment']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(18,22) :: image-based quality assessment approach', '(21,22) :: approach', '(34,36) :: model-based metrics', '(51,52) :: applications', '(18,21) :: image-based quality assessment']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 541 recall: 32/35 = 0.914\n",
      "==============================================================================================================\n",
      "[22/30] Topic 542 — paragraphs: 54\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "In the past decades, different kinds of metaheuristic optimization algorithms [1, 2] have been developed; Simulated Annealing (SA) [3, 4], Evolutionary Algorithms (EAs) [5–7], Differential Evolution (DE) [8, 9], Particle Swarm Optimization (PSO) [10, 11], Ant Colony Optimization (ACO) [12, 13], and Estimation of Distribution Algorithms (EDAs) [14, 15] are just a few of them. These algorithms have shown excellent search abilities but often lose their efficacy when applied to large and complex problems, e.g., problem instances with high dimensions, such as those with more than one hundred decision variables.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['Differential Evolution']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['metaheuristic optimization algorithms', '1, 2] have been developed; [ Simulated Annealing (SA)', '3, 4], [ Evolutionary Algorithms (EAs)', '8, 9], [ Particle Swarm Optimization (PSO)', '10, 13], and [ Estimation of Distribution Algorithms (EDAs)', '14, 15] are just a few of them. These [ algorithms', 'problem instances', 'those', 'decision variables']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(8,11) :: metaheuristic optimization algorithms', '(117,119) :: problem instances', '(125,126) :: those', '(131,133) :: decision variables']\n",
      "\n",
      "HITS=0  MISSES=1\n",
      "Missed gold (idx, span, text):\n",
      "  - #0 (40,41) :: Differential Evolution\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "The proposed SABEA adopts two bounding strategies, namely fitness-based bounding and probabilistic sampling-based bounding, to select a set of individuals over multiple generations and leverage the value information from these individuals to update the search space of a given problem for improving the solution accuracy and search efficiency. To evaluate the performance of this method, SABEA is applied on top of the classic differential evolution (DE) algorithm and a DE variant, and SABEA is compared to a state-of-the-art Distribution-based Adaptive Bounding Genetic Algorithm (DABGA) on a set of 27 selected benchmark functions. The results show that SABEA can be used as a complementary strategy for further enhancing the performance of existing evolutionary algorithms and it also outperforms DABGA.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['differential evolution (DE) algorithm']\n",
      "\n",
      "[PRED mentions (strings)] n= 13\n",
      "['SABEA', 'bounding strategies', 'fitness-based bounding', 'probabilistic sampling-based bounding', 'method', 'SABEA', 'differential evolution (DE) algorithm', 'DE variant', 'SABEA', 'Distribution-based Adaptive Bounding Genetic Algorithm (DABGA)', 'SABEA', 'it']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(2,3) :: SABEA', '(5,7) :: bounding strategies', '(9,11) :: fitness-based bounding', '(12,15) :: probabilistic sampling-based bounding', '(57,58) :: method', '(67,73) :: differential evolution (DE) algorithm', '(75,77) :: DE variant', '(85,93) :: Distribution-based Adaptive Bounding Genetic Algorithm (DABGA)', '(124,125) :: it', '(91,92) :: DABGA']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "Control parameters and mutation methods impact upon the global search ability of differential evolution algorithm (DE), and varying optimization issues own varying parameter settings. In this paper, an enhanced elite archive mutation strategy with self-adaption parameter adjustment (EAMSADE) is proposed to raise DE ’s performance.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['differential evolution algorithm (DE)']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['Control parameters', 'mutation methods', 'global search ability', 'differential evolution algorithm (DE)', 'optimization issues', 'parameter settings', 'elite archive mutation strategy', 'self-adaption parameter adjustment (EAMSADE)', 'DE']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,2) :: Control parameters', '(3,5) :: mutation methods', '(8,11) :: global search ability', '(12,18) :: differential evolution algorithm (DE)', '(21,23) :: optimization issues', '(25,27) :: parameter settings', '(34,38) :: elite archive mutation strategy', '(39,45) :: self-adaption parameter adjustment (EAMSADE)', '(16,17) :: DE']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Six benchmark functions have been employed to test the performance of HRO. In addition, particle swarm optimization algorithm (PSO), differential evolution algorithm(DE), genetic algorithm (GA), artificial bee colony (ABC) are also carried out to compare with the HRO in the same running environment. Experimental results on some benchmark problems verifies that HRO outperforms other optimization algorithms involved in the paper with respect to convergence and the speed of calculation on the whole.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['differential evolution algorithm(DE)']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['benchmark functions', 'HRO', 'particle swarm optimization algorithm (PSO) differential evolution algorithm(DE) genetic algorithm (GA) artificial bee colony (ABC)', 'HRO', 'HRO']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(1,3) :: benchmark functions', '(11,12) :: HRO']\n",
      "\n",
      "HITS=0  MISSES=1\n",
      "Missed gold (idx, span, text):\n",
      "  - #0 (24,27) :: differential evolution algorithm(DE)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 542 recall: 48/55 = 0.873\n",
      "==============================================================================================================\n",
      "[23/30] Topic 543 — paragraphs: 54\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "For unsupervised domain adaptation problems, the strategy of aligning the two domains in latent feature space through adversarial learning has achieved much progress in image classification, but usually fails in semantic segmentation tasks in which the latent representations are overcomplex. In this work, we equip the adversarial network with a \" significance-aware information bottleneck (SIB) \", to address the above problem.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['semantic segmentation tasks']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['unsupervised domain adaptation problems', 'latent feature space', 'adversarial learning', 'image classification', 'semantic segmentation tasks', 'latent representations', 'adversarial network', 'significance-aware information bottleneck (SIB)', 'significance-aware information bottleneck (SIB)']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(1,5) :: unsupervised domain adaptation problems', '(14,17) :: latent feature space', '(18,20) :: adversarial learning', '(25,27) :: image classification', '(32,35) :: semantic segmentation tasks', '(38,40) :: latent representations', '(50,52) :: adversarial network', '(55,61) :: significance-aware information bottleneck (SIB)']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "Targeted on maize, we proposed to jointly segment crop and maize tassel. The task is consequently formulated as a semantic segmentation problem. We proposed a region-based approach that leverages the efficient graph-based segmentation algorithm and simple linear iterative clustering (SLIC) to generate region proposals.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['semantic segmentation problem']\n",
      "\n",
      "[PRED mentions (strings)] n= 6\n",
      "['task', 'semantic segmentation problem', 'region-based approach', 'graph-based segmentation algorithm', 'linear iterative clustering ( SLIC)', 'region proposals']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(15,16) :: task', '(21,24) :: semantic segmentation problem', '(28,30) :: region-based approach', '(34,37) :: graph-based segmentation algorithm', '(47,49) :: region proposals']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "However, vast majority of the existing methods detect text within local regions, typically through extracting character, word or line level candidates followed by candidate aggregation and false positive elimination, which potentially exclude the effect of wide-scope and long-range contextual cues in the scene. To take full advantage of the rich information available in the whole natural image, we propose to localize text in a holistic manner, by casting scene text detection as a semantic segmentation problem. The proposed algorithm directly runs on full images and produces global, pixel-wise prediction maps, in which detections are subsequently formed.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['semantic segmentation problem']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['methods', 'character, word or line level candidates', 'candidate aggregation', 'false positive elimination', 'wide-scope and long-range contextual cues', 'semantic segmentation problem', 'algorithm', 'full images', 'global , pixel-wise prediction maps', 'detections']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(7,8) :: methods', '(17,24) :: character, word or line level candidates', '(26,28) :: candidate aggregation', '(29,32) :: false positive elimination', '(39,44) :: wide-scope and long-range contextual cues', '(80,83) :: semantic segmentation problem', '(86,87) :: algorithm', '(90,92) :: full images', '(102,103) :: detections']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "The proposed dataset consists of 4 relevant classes to inspect the endoluminal scene, targeting different clinical needs. Together with the dataset and taking advantage of advances in semantic segmentation literature, we provide new baselines by training standard fully convolutional networks (FCNs). We perform a comparative study to show that FCNs significantly outperform, without any further postprocessing, prior results in endoluminal scene segmentation, especially with respect to polyp segmentation and localization.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['semantic segmentation literature']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['dataset', 'endoluminal scene', 'dataset', 'semantic segmentation literature', 'fully convolutional networks (FCNs', 'FCNs', 'postprocessing', 'endoluminal scene segmentation', 'polyp segmentation', 'localization']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(2,3) :: dataset', '(11,13) :: endoluminal scene', '(29,32) :: semantic segmentation literature', '(40,45) :: fully convolutional networks (FCNs', '(44,45) :: FCNs', '(62,63) :: postprocessing', '(67,70) :: endoluminal scene segmentation', '(75,77) :: polyp segmentation', '(78,79) :: localization']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 543 recall: 46/54 = 0.852\n",
      "==============================================================================================================\n",
      "[24/30] Topic 544 — paragraphs: 32\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "The existing Long Short-Term Memory methods for traffic prediction have two drawbacks: they do not use the departure time through the links for traffic prediction, and the way of modeling long-term dependence in time series is not direct in terms of traffic prediction. Attention mechanism is implemented by constructing a neural network according to its task and has recently demonstrated success in a wide range of tasks. In this paper, we propose an Long Short-Term Memory-based method with attention mechanism for travel time prediction.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['Attention mechanism']\n",
      "\n",
      "[PRED mentions (strings)] n= 11\n",
      "['Long Short-Term Memory methods', 'traffic prediction', 'they', 'departure time', 'traffic prediction', 'traffic prediction', 'Attention mechanism', 'neural network', 'Long Short-Term Memory-based method', 'attention mechanism', 'travel time prediction']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(2,6) :: Long Short-Term Memory methods', '(7,9) :: traffic prediction', '(13,14) :: they', '(18,20) :: departure time', '(46,48) :: Attention mechanism', '(53,55) :: neural network', '(78,82) :: Long Short-Term Memory-based method', '(86,89) :: travel time prediction']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "We propose a variant of the Transformer network which we call Weighted Transformer that uses self - attention branches in lieu of the multi - head attention. The branches replace the multiple heads in the attention mechanism of the original Transformer network, and the model learns to combine these branches during training. This branched architecture enables the network to achieve comparable performance at a significantly lower computational cost.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['attention mechanism']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['Transformer network', 'Weighted Transformer', 'self - attention branches', 'branches', 'Transformer network', 'model', 'branches', 'network', 'computational cost']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(6,8) :: Transformer network', '(11,13) :: Weighted Transformer', '(15,19) :: self - attention branches', '(18,19) :: branches', '(46,47) :: model', '(7,8) :: network', '(69,71) :: computational cost']\n",
      "\n",
      "HITS=0  MISSES=1\n",
      "Missed gold (idx, span, text):\n",
      "  - #0 (36,37) :: attention mechanism\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "This imposes practical difficulties when processing long sequences. We address this issue with attention mechanism over tree. In addition, the attention mechanism can be used for matching trees (described in Section 4 as Tree matching NTI) that carry different sequence information.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['attention mechanism']\n",
      "\n",
      "[PRED mentions (strings)] n= 6\n",
      "['attention mechanism', 'tree', 'attention mechanism', 'matching trees', 'Tree matching NTI)', 'sequence information']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(14,16) :: attention mechanism', '(17,18) :: tree', '(29,31) :: matching trees', '(37,41) :: Tree matching NTI)', '(44,46) :: sequence information']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "It has obtained considerable attention due to its broad applications in natural language processing. Most existing studies set up sentiment classifiers using supervised machine learning approaches, such as support vector machine (SVM), convolutional neural network (CNN), long short - term memory (LSTM), Tree - LSTM, and attention - based methods. Despite the remarkable progress made by the previous work, we argue that sentiment analysis still remains a challenge.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['attention - based methods']\n",
      "\n",
      "[PRED mentions (strings)] n= 6\n",
      "['It', 'natural language processing', 'sentiment classifiers', 'supervised machine learning approaches', 'support vector machine (SVM), convolutional neural network (CNN) long short - term memory (LSTM) Tree - LSTM ] , and [ attention - based methods', 'sentiment analysis']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,1) :: It', '(11,14) :: natural language processing', '(20,22) :: sentiment classifiers', '(23,27) :: supervised machine learning approaches', '(76,78) :: sentiment analysis']\n",
      "\n",
      "HITS=0  MISSES=1\n",
      "Missed gold (idx, span, text):\n",
      "  - #0 (58,61) :: attention - based methods\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 544 recall: 24/32 = 0.750\n",
      "==============================================================================================================\n",
      "[25/30] Topic 545 — paragraphs: 29\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "Furthermore, we successfully re-use one of the decoders used in translation for segmentation. We validate the proposed method on synthetic tasks of varying difficulty as well as on the real task of brain tumor segmentation in magnetic resonance images, where we show significant improvements over standard semi-supervised training with autoencoding.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['autoencoding']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['decoders', 'translation', 'segmentation', 'method', 'synthetic tasks', 'brain tumor segmentation', 'magnetic resonance images', 'semi-supervised training', 'autoencoding']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(8,9) :: decoders', '(11,12) :: translation', '(13,14) :: segmentation', '(19,20) :: method', '(21,23) :: synthetic tasks', '(34,37) :: brain tumor segmentation', '(38,41) :: magnetic resonance images', '(49,51) :: semi-supervised training', '(52,53) :: autoencoding']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "Technically, we show that the resultant optimization problem can be efficiently solved by maximizing a variational lower-bound of the mutual information. This variational approach introduces a transformation decoder to approximate the intractable posterior of transformations, resulting in an autoencoding architecture with a pair of the representation encoder and the transformation decoder. Experiments demonstrate the proposed AVT model sets a new record for the performances on unsupervised tasks, greatly closing the performance gap to the supervised models.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['autoencoding architecture']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['variational lower-bound of the mutual information', 'variational approach', 'transformation decoder', 'intractable posterior of transformations', 'autoencoding architecture', 'representation encoder', 'transformation decoder', 'AVT model', 'unsupervised tasks', 'supervised models']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(16,22) :: variational lower-bound of the mutual information', '(24,26) :: variational approach', '(28,30) :: transformation decoder', '(33,37) :: intractable posterior of transformations', '(41,43) :: autoencoding architecture', '(48,50) :: representation encoder', '(59,61) :: AVT model', '(69,71) :: unsupervised tasks', '(79,81) :: supervised models']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "Each embedding is a vector that represents four contiguous beats of music and is derived from a symbolic representation. We consider autoencoding-based methods including denoising autoencoders, and context reconstruction, and evaluate the resulting embeddings on a forward prediction and a classification task.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['autoencoding-based methods']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['vector', 'contiguous beats of music', 'symbolic representation', 'autoencoding-based methods', 'denoising autoencoders', 'context reconstruction', 'embeddings', 'classification task']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(4,5) :: vector', '(8,12) :: contiguous beats of music', '(17,19) :: symbolic representation', '(22,24) :: autoencoding-based methods', '(25,27) :: denoising autoencoders', '(29,31) :: context reconstruction', '(36,37) :: embeddings', '(43,45) :: classification task']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "This paper addresses the problem of learning object structures in an image modeling process without supervision. We propose an autoencoding formulation to discover landmarks as explicit structural representations. The encoding module outputs landmark coordinates, whose validity is ensured by constraints that reflect the necessary properties for landmarks.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['autoencoding formulation']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['learning object structures', 'image modeling process', 'autoencoding formulation', 'landmarks', 'structural representations', 'encoding module', 'landmark coordinates', 'landmarks']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(6,9) :: learning object structures', '(11,14) :: image modeling process', '(20,22) :: autoencoding formulation', '(24,25) :: landmarks', '(27,29) :: structural representations', '(31,33) :: encoding module', '(34,36) :: landmark coordinates']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 545 recall: 25/29 = 0.862\n",
      "==============================================================================================================\n",
      "[26/30] Topic 546 — paragraphs: 44\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "Attention based end-to-end speech synthesis achieves better performance in both prosody and quality compared to the conventional \" front-end\"-\"back-end \" structure. But training such end-to-end framework is usually time-consuming because of the use of recurrent neural networks. To enable parallel calculation and long-range dependency modeling, a solely self-attention based framework named Transformer is proposed recently in the end-to-end family.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['recurrent neural networks']\n",
      "\n",
      "[PRED mentions (strings)] n= 6\n",
      "['Attention based end-to-end speech synthesis', 'front-end\"-\"back-end \" structure', 'end-to-end framework', 'recurrent neural networks', 'parallel calculation', 'long-range dependency modeling']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,5) :: Attention based end-to-end speech synthesis', '(18,21) :: front-end\"-\"back-end \" structure', '(25,27) :: end-to-end framework', '(35,38) :: recurrent neural networks', '(41,43) :: parallel calculation', '(44,47) :: long-range dependency modeling']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "The paper provides an overview of modern recurrent neural network architectures that can be used as a model (building tool) for diagnosis and predicting of infocommunication systems elements. The paper presents arguments in favor of the using of recurrent neural networks.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['recurrent neural network architectures']\n",
      "\n",
      "[PRED mentions (strings)] n= 4\n",
      "['recurrent neural network architectures', 'model (building tool)', 'diagnosis and predicting of infocommunication systems elements', 'recurrent neural networks']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(7,11) :: recurrent neural network architectures', '(17,22) :: model (building tool)', '(23,30) :: diagnosis and predicting of infocommunication systems elements', '(41,44) :: recurrent neural networks']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "Insufficient level of geographic information in nucleotide sequence repositories such as GenBank motivates the use of natural language processing methods for extracting geographic location names (toponyms) in the scientific article associated with the sequence, and disambiguating the locations to their co-ordinates. In this paper, we present an extensive study of multiple recurrent neural network architectures for the task of extracting geographic locations and their effective contribution to the disambiguation task using population heuristics. The methods presented in this paper achieve a strict detection F1 score of 0.94, disambiguation accuracy of 91% and an overall resolution F1 score of 0.88 that are significantly higher than previously developed methods, improving our capability to find the location of infected hosts and enrich metadata information.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['recurrent neural network architectures']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['geographic information', 'nucleotide sequence repositories', 'GenBank', 'natural language processing methods', 'geographic location names ( toponyms)', 'recurrent neural network architectures', 'disambiguation task', 'methods', 'detection F1 score', 'disambiguation accuracy']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(3,5) :: geographic information', '(6,9) :: nucleotide sequence repositories', '(11,12) :: GenBank', '(16,20) :: natural language processing methods', '(56,60) :: recurrent neural network architectures', '(73,75) :: disambiguation task', '(19,20) :: methods', '(88,91) :: detection F1 score', '(94,96) :: disambiguation accuracy']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word - by - word attention mechanism to acquire question - aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top - ranked chunk as the answer. Experimental results show that DCR achieves state - of - the - art exact match and F1 scores on the SQuAD dataset.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['recurrent neural networks']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['DCR', 'neural RC models', 'DCR', 'recurrent neural networks', 'question - aware representations', 'DCR', 'F1 scores', 'SQuAD dataset']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,1) :: DCR', '(12,15) :: neural RC models', '(33,36) :: recurrent neural networks', '(50,54) :: question - aware representations', '(96,98) :: F1 scores', '(100,102) :: SQuAD dataset']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 546 recall: 41/49 = 0.837\n",
      "==============================================================================================================\n",
      "[27/30] Topic 547 — paragraphs: 73\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "Finally LSBN employs an efficient algorithm, to merge structures of overlapping communities into a whole. In comparison with other four state-of-art large-scale network structure learning algorithms such as ARACNE, PC, Greedy Search and MMHC, LSBN shows comparable results in five common benchmark datasets, evaluated by precision, recall and f-score. What 's more, LSBN makes it possible to learn large-scale Bayesian structure by Model Averaging which used to be intractable.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['Greedy Search']\n",
      "\n",
      "[PRED mentions (strings)] n= 10\n",
      "['LSBN', 'algorithm', 'large-scale network structure learning algorithms', 'ARACNE', 'PC', 'MMHC', 'precision', 'f-score', 'LSBN', 'large-scale Bayesian structure']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(1,2) :: LSBN', '(5,6) :: algorithm', '(23,28) :: large-scale network structure learning algorithms', '(30,31) :: ARACNE', '(32,33) :: PC', '(37,38) :: MMHC', '(51,52) :: precision', '(55,56) :: f-score', '(67,70) :: large-scale Bayesian structure']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "However, the relaxation can only be applied to two-class cases. In this paper, we propose full eigenvector analysis of p-Laplacian and obtain a natural global embedding for multi-class clustering problems, instead of using greedy search strategy implemented by previous researchers. An efficient gradient descend optimization approach is introduced to obtain the p-Laplacian embedding space, which is guaranteed to converge to feasible local solutions.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['greedy search strategy']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['relaxation', 'full eigenvector analysis of p-Laplacian', 'natural global embedding', 'multi-class clustering problems', 'greedy search strategy', 'gradient descend optimization approach', 'p-Laplacian embedding space']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(3,4) :: relaxation', '(18,23) :: full eigenvector analysis of p-Laplacian', '(26,29) :: natural global embedding', '(30,33) :: multi-class clustering problems', '(37,40) :: greedy search strategy', '(47,51) :: gradient descend optimization approach', '(56,59) :: p-Laplacian embedding space']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "Even under the worst case condition, the proposed method can potentially increase the expected rate of successful searches by 100%. Moreover, the proposed search strategy outperforms a greedy search strategy, that considers only the users ' location probabilities and ignores their deadline constraints. Under certain conditions, the expected rate of successful searches generated by the proposed method is twice the equivalent rate generated by the greedy search strategy.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['greedy search strategy']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['method', 'search strategy', 'greedy search strategy', 'method', 'greedy search strategy']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(9,10) :: method', '(27,29) :: search strategy', '(31,34) :: greedy search strategy']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Lastly, we perform repositioning of drugs to the top 20% ranked diseases. ResultsThe results showed that F-measure of the proposed method was 0.75, outperforming 0.5 of greedy searching for the entire diseases. For the utility of the proposed method, it was applied to dementia and verified 75% accuracy for repositioned drugs assuming that there are not any known drugs to be used for dementia.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['greedy searching']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['repositioning of drugs', 'F-measure', 'method', 'method', 'it', 'dementia', 'accuracy', 'repositioned drugs', 'dementia']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(4,7) :: repositioning of drugs', '(19,20) :: F-measure', '(23,24) :: method', '(45,46) :: it', '(49,50) :: dementia', '(54,55) :: accuracy', '(56,58) :: repositioned drugs']\n",
      "\n",
      "HITS=0  MISSES=1\n",
      "Missed gold (idx, span, text):\n",
      "  - #0 (30,31) :: greedy searching\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 547 recall: 54/75 = 0.720\n",
      "==============================================================================================================\n",
      "[28/30] Topic 548 — paragraphs: 75\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "Some approaches have been proposed to introduce new technologies in the domain. Recently there have been proposals to have a thin client approach to network management. Other suggestions have been to use mobile code for delegation.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['network management']\n",
      "\n",
      "[PRED mentions (strings)] n= 3\n",
      "['thin client approach', 'network management', 'mobile code']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(21,24) :: thin client approach', '(25,27) :: network management', '(34,36) :: mobile code']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "The Internet Network Management Workshop is working to build a community of researchers interested in solving the challenges of network management via a combination of bottoms-up analysis of data from existing networks and a top-down design of new architectures and approaches driven by that data. This editorial sets out some of the research challenges we see facing network management, and calls for participation in working to solve them.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['network management']\n",
      "\n",
      "[PRED mentions (strings)] n= 4\n",
      "['Internet Network Management Workshop', 'network management', 'bottoms-up analysis of data', 'facing network management']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(1,5) :: Internet Network Management Workshop', '(2,4) :: network management', '(25,29) :: bottoms-up analysis of data', '(57,60) :: facing network management']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "While the current world wide network offers an incredibly rich base of information, it causes network management problem because users should have many independent IDs and passwords for accessing different sewers located in many places. In order to solve this problem users have employed single circle of trust(COT) ID management system, but it is still not sufficient for clearing the problem because the coming ubiquitous network computing environment will be integrated and complex networks combined with wired and wireless network devices.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['network management problem']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['world wide network', 'it', 'network management problem', 'problem', 'single circle of trust(COT) ID management system', 'it', 'problem']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(3,6) :: world wide network', '(14,15) :: it', '(16,19) :: network management problem', '(18,19) :: problem', '(46,54) :: single circle of trust(COT) ID management system']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "As a new approach slow intelligence system is proposed to manage the network problem. The architecture and special feature of SIS is to improve the result and get the feasible solution for the network management problem through a process involving such as enumeration, propagation, adaptation, elimination and concentration. Steps of initial experimental result are also focused.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['network management problem']\n",
      "\n",
      "[PRED mentions (strings)] n= 8\n",
      "['approach slow intelligence system', 'network problem', 'SIS', 'network management problem', 'enumeration', 'propagation', 'adaptation', 'concentration']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(3,7) :: approach slow intelligence system', '(12,14) :: network problem', '(21,22) :: SIS', '(34,37) :: network management problem', '(43,44) :: enumeration', '(45,46) :: propagation', '(47,48) :: adaptation', '(51,52) :: concentration']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 548 recall: 65/75 = 0.867\n",
      "==============================================================================================================\n",
      "[29/30] Topic 549 — paragraphs: 47\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "Recently a tree structure model for multi-view face detection was proposed. This method is primarily designed for facial landmark detection and consequently a face detection is provided. However, the effort to model inner facial structures by using a detailed facial landmark labelling resulted on a complex and suboptimal system for face detection.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['facial landmark detection']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['tree structure model', 'multi-view face detection', 'method', 'facial landmark detection', 'face detection', 'facial landmark labelling', 'face detection']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(2,5) :: tree structure model', '(6,9) :: multi-view face detection', '(13,14) :: method', '(18,21) :: facial landmark detection', '(7,9) :: face detection', '(42,45) :: facial landmark labelling']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "Facial landmark detection, as a typical and crucial task in computer vision, is widely used in face recognition, face animation, facial expression analysis, etc. In the past decades, many efforts are devoted to designing robust facial landmark detection algorithms.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['Facial landmark detection']\n",
      "\n",
      "[PRED mentions (strings)] n= 6\n",
      "['Facial landmark detection', 'computer vision', 'face recognition', 'face animation', 'facial expression analysis', 'facial landmark detection algorithms']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,3) :: Facial landmark detection', '(11,13) :: computer vision', '(18,20) :: face recognition', '(21,23) :: face animation', '(24,27) :: facial expression analysis', '(42,46) :: facial landmark detection algorithms']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "However, such techniques have shown low accuracy, especially in the exceptional conditions such as low luminance or overlapped face. To overcome this problem, we propose a new facial landmark extraction scheme using deep learning and semantic segmentation and demonstrate that with even small dataset, our scheme can achieve excellent facial landmark extraction performance.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['facial landmark extraction']\n",
      "\n",
      "[PRED mentions (strings)] n= 9\n",
      "['techniques', 'accuracy', 'low luminance', 'overlapped face', 'facial landmark extraction scheme', 'deep learning', 'semantic segmentation', 'scheme', 'facial landmark extraction']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(3,4) :: techniques', '(7,8) :: accuracy', '(16,18) :: low luminance', '(19,21) :: overlapped face', '(31,35) :: facial landmark extraction scheme', '(36,38) :: deep learning', '(39,41) :: semantic segmentation', '(34,35) :: scheme', '(31,34) :: facial landmark extraction']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "Hence, the quantification of the error associated to the location of facial landmarks seems to be necessary when photographs become a key element of the forensic procedure. In this work, we statistically evaluate the inter- and intra-observer dispersions related to the facial landmark identification on photographs. In the inter-observer experiment, a set of 18 facial landmarks was provided to 39 operators.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['facial landmark identification']\n",
      "\n",
      "[PRED mentions (strings)] n= 5\n",
      "['facial landmarks', 'forensic procedure', 'inter- and intra-observer dispersions', 'facial landmark identification', 'facial landmarks']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(12,14) :: facial landmarks', '(26,28) :: forensic procedure', '(37,41) :: inter- and intra-observer dispersions', '(44,47) :: facial landmark identification']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 549 recall: 46/48 = 0.958\n",
      "==============================================================================================================\n",
      "[30/30] Topic 550 — paragraphs: 51\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Paragraph 0 (doc_id=0) ---\n",
      "[TEXT]\n",
      "In this paper, a critical situation learning module is combined with the evolved fuzzy systems, i.e., reflective action module. The critical situation learning module is composed of Q-learning with CMAC. Location information of surrounding ghosts and the existence of power-pills are given to PacMan as state.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['Q-learning']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['critical situation learning module', 'fuzzy systems', 'reflective action module', 'critical situation learning module', 'Q-learning', 'CMAC. Location information of surrounding ghosts', 'power-pills']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(5,9) :: critical situation learning module', '(14,16) :: fuzzy systems', '(19,22) :: reflective action module', '(31,32) :: Q-learning', '(33,40) :: CMAC. Location information of surrounding ghosts', '(44,45) :: power-pills']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 1 (doc_id=1) ---\n",
      "[TEXT]\n",
      "Q -- learning algorithm is one of the most popular model -- free reinforcement learning algorithms. This paper extends Q -- learning algorithm to multi -- agent cooperative team domain properly, and proposes a shared experience tuples multi -- agent cooperative reinforcement learning algorithm, in which a new knowledge representation form composed of sequential pair as 〈 state-- value, action -- valu 〉 is proposed. And experience tuples are shared with other agents through similarity transformation according to homogeneous subtasks.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['Q -- learning algorithm']\n",
      "\n",
      "[PRED mentions (strings)] n= 7\n",
      "['Q -- learning algorithm', 'free reinforcement learning algorithms', 'Q -- learning algorithm', 'agent cooperative team domain', 'shared experience tuples multi -- agent cooperative reinforcement learning algorithm', 'knowledge representation form', 'homogeneous subtasks']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(0,4) :: Q -- learning algorithm', '(12,16) :: free reinforcement learning algorithms', '(27,31) :: agent cooperative team domain', '(36,46) :: shared experience tuples multi -- agent cooperative reinforcement learning algorithm', '(51,54) :: knowledge representation form', '(83,85) :: homogeneous subtasks']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 2 (doc_id=2) ---\n",
      "[TEXT]\n",
      "We prove that neural Q-learning finds the optimal policy with $ O(1/\\sqrt{T})$ convergence rate if the neural function approximator is sufficiently overparameterized, where $ T$ is the number of iterations. To our best knowledge, our result is the first finite-time analysis of neural Q-learning under non-i.i.d. data assumption.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['neural Q-learning']\n",
      "\n",
      "[PRED mentions (strings)] n= 6\n",
      "['neural Q-learning', 'policy', 'O(1/sqrtT)$ convergence rate', 'neural function approximator', 'finite-time analysis of neural Q-learning', 'non-i.i.d. data assumption']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(3,5) :: neural Q-learning', '(8,9) :: policy', '(16,19) :: neural function approximator', '(42,47) :: finite-time analysis of neural Q-learning', '(48,52) :: non-i.i.d. data assumption']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--- Paragraph 3 (doc_id=3) ---\n",
      "[TEXT]\n",
      "In this work we present the results of preliminary studies on how neural networks can be utilized to path planning on square grids, e.g. how well they can cope with path finding tasks by themselves within the well-known reinforcement problem statement. Conducted experiments show that the agent using neural Q-learning algorithm robustly learns to achieve the goal on small maps and demonstrate promising results on the maps have ben never seen by him before.\n",
      "\n",
      "[GOLD mentions] n= 1\n",
      "['neural Q-learning algorithm']\n",
      "\n",
      "[PRED mentions (strings)] n= 6\n",
      "['neural networks', 'path planning on square grids', 'they', 'path finding tasks', 'reinforcement problem statement', 'neural Q-learning algorithm']\n",
      "\n",
      "[PRED spans (first match per string) — for inspection]\n",
      "['(12,14) :: neural networks', '(18,23) :: path planning on square grids', '(27,28) :: they', '(31,34) :: path finding tasks', '(39,42) :: reinforcement problem statement', '(50,53) :: neural Q-learning algorithm']\n",
      "\n",
      "HITS=1  MISSES=0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Topic 550 recall: 46/52 = 0.885\n",
      "\n",
      "==============================================================================================================\n",
      "[MICRO] recall over first 30 topics: 0.831  (1288/1550)\n",
      "[MACRO] recall over first 30 topics: 0.837\n"
     ]
    }
   ],
   "source": [
    "topic_ids = order_topic_ids[:K]\n",
    "\n",
    "total_gold = total_hits = 0\n",
    "macro_stats = []\n",
    "\n",
    "for t_idx, tid in enumerate(topic_ids, 1):\n",
    "    # number of paragraphs for this topic\n",
    "    n_paras = sum(1 for (t,p) in gold.keys() if t == tid)\n",
    "    print(\"=\"*110)\n",
    "    print(f\"[{t_idx}/{len(topic_ids)}] Topic {tid} — paragraphs: {n_paras}\")\n",
    "    print(\"-\"*110)\n",
    "\n",
    "    topic_gold = topic_hits = 0\n",
    "\n",
    "    # show first few paragraphs verbosely\n",
    "    for pid in range(min(SHOW_FIRST_N_PARAS, n_paras)):\n",
    "        res = compare_paragraph_fuzzy(tid, pid)\n",
    "        print(f\"\\n--- Paragraph {pid} (doc_id={res.get('doc_id','?')}) ---\")\n",
    "        if res[\"status\"] != \"ok\":\n",
    "            print(res[\"status\"])\n",
    "            continue\n",
    "\n",
    "        print(\"[TEXT]\")\n",
    "        print(res[\"text\"])\n",
    "\n",
    "        print(\"\\n[GOLD mentions] n=\", res[\"n_gold\"])\n",
    "        print(res[\"gold_strings\"][:SHOW_FIRST_N_PRED])\n",
    "\n",
    "        print(\"\\n[PRED mentions (strings)] n=\", len(res[\"pred_strings\"]))\n",
    "        print(res[\"pred_strings\"][:SHOW_FIRST_N_PRED])\n",
    "\n",
    "        print(\"\\n[PRED spans (first match per string) — for inspection]\")\n",
    "        show = []\n",
    "        for s, (st,en) in res[\"pred_spans_display\"][:SHOW_FIRST_N_PRED]:\n",
    "            show.append(f\"({st},{en}) :: {s}\")\n",
    "        print(show)\n",
    "\n",
    "        print(f\"\\nHITS={res['hits']}  MISSES={len(res['misses'])}\")\n",
    "        if res[\"misses\"]:\n",
    "            print(\"Missed gold (idx, span, text):\")\n",
    "            for gidx, (s,e), gtext in res[\"misses\"][:SHOW_FIRST_N_PRED]:\n",
    "                print(f\"  - #{gidx} ({s},{e}) :: {gtext}\")\n",
    "\n",
    "        topic_gold += res[\"n_gold\"]\n",
    "        topic_hits += res[\"hits\"]\n",
    "\n",
    "    # aggregate over *all* paragraphs in the topic for recall (not just printed ones)\n",
    "    for pid in range(n_paras):\n",
    "        res = compare_paragraph_fuzzy(tid, pid)\n",
    "        if res[\"status\"] != \"ok\":\n",
    "            continue\n",
    "        topic_gold += 0  # already added in verbose loop? Let's recompute cleanly below.\n",
    "\n",
    "    # clean recompute over all paras for topic recall\n",
    "    topic_gold = topic_hits = 0\n",
    "    for pid in range(n_paras):\n",
    "        r = compare_paragraph_fuzzy(tid, pid)\n",
    "        if r[\"status\"] != \"ok\":\n",
    "            continue\n",
    "        topic_gold += r[\"n_gold\"]\n",
    "        topic_hits += r[\"hits\"]\n",
    "\n",
    "    topic_recall = (topic_hits / topic_gold) if topic_gold else 0.0\n",
    "    total_gold += topic_gold\n",
    "    total_hits += topic_hits\n",
    "    macro_stats.append((topic_hits, topic_gold))\n",
    "\n",
    "    print(\"\\n\" + \"-\"*110)\n",
    "    print(f\"Topic {tid} recall: {topic_hits}/{topic_gold} = {topic_recall:.3f}\")\n",
    "\n",
    "micro_recall = (total_hits / total_gold) if total_gold else 0.0\n",
    "macro_recall = sum(h/g for h,g in macro_stats if g>0) / max(1, sum(1 for _,g in macro_stats if g>0))\n",
    "print(\"\\n\" + \"=\"*110)\n",
    "print(f\"[MICRO] recall over first {K} topics: {micro_recall:.3f}  ({total_hits}/{total_gold})\")\n",
    "print(f\"[MACRO] recall over first {K} topics: {macro_recall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7209664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "188ff9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topics: 100%|██████████| 100/100 [00:28<00:00,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SciCo split: validation\n",
      "Topics evaluated: 100  |  Total gold mentions: 4873\n",
      "[MICRO] recall: 0.8211  (4001/4873)\n",
      "[MACRO] recall: 0.8237\n",
      "Missed cases log: _.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluate_tanl import *\n",
    "\n",
    "k = -1\n",
    "split = \"validation\"\n",
    "pred_path = \"scico_dev_tanl_extraction.jsonl\"\n",
    "log_path = \"_.txt\"\n",
    "delta_pos = 2\n",
    "delta_len = 2\n",
    "\n",
    "_, gold, topic_order = load_scico_gold(split)\n",
    "pred = load_predictions(pred_path)\n",
    "\n",
    "# choose topic subset\n",
    "topic_ids = topic_order if k < 0 else topic_order[:k]\n",
    "\n",
    "# progress & logging\n",
    "total_gold = total_hits = 0\n",
    "macro_parts = []\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as log_fh:\n",
    "    for idx, tid in enumerate(tqdm(topic_ids, desc=\"Evaluating topics\")):\n",
    "        th, tg, rec = eval_topic(\n",
    "            tid, gold, pred, delta_pos, delta_len, log_fh=log_fh\n",
    "        )\n",
    "        total_hits += th\n",
    "        total_gold += tg\n",
    "        if tg > 0:\n",
    "            macro_parts.append(th / tg)\n",
    "\n",
    "micro_recall = (total_hits / total_gold) if total_gold else 0.0\n",
    "macro_recall = (sum(macro_parts) / len(macro_parts)) if macro_parts else 0.0\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"SciCo split: {split}\")\n",
    "print(f\"Topics evaluated: {len(topic_ids)}  |  Total gold mentions: {total_gold}\")\n",
    "print(f\"[MICRO] recall: {micro_recall:.4f}  ({total_hits}/{total_gold})\")\n",
    "print(f\"[MACRO] recall: {macro_recall:.4f}\")\n",
    "print(f\"Missed cases log: {log_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-scico_cdcr (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
